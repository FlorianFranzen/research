{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Polkadot DRAFT This article summarizes the research efforts relevant to the Polkadot project, for other information regarding the project please refer to the wiki page . Polkadot speaking in abstract terms provides a number of connected canonical state machines. Connected means that a state transition of one machine can affect a transition of another machine. The state machines are canonical, since they transition in a globally consistent manner. We would also like to enable adding, removing and changing of the state machines as the time goes on. This will be the role of the governance process. The research focuses on how to enable having such publicly available system in the face of possible adversarial conditions. The public can use the system by interacting with state machines that they are interested in via the internet. Each state machine can provide different functionalities and behave in different ways (have a different state and state transition scheme). So let us start with abstract state machines. A state machine has a certain state type and state transition type. As the time goes on, state transitions occur. The data that determines the state transitions is structured as bundles of transactions - individual small state transitions triggered by the users of the system. Each bundle is called a block. In order to achieve its properties, ensures that those blocks are hash connected forming joint data structure. 1. Specification of the Polkadot Runtime Environment We are working on a implementation level specification of the protocol here . 2. Identifying participants to run the network 2.1 Keys To identify unique individual participants that will perform duties on the network we use public key cryptography. You can read more about our approach here and see the particular crypto for the first implementation in the Schnorrkel repo . Validator keys indicated by the staking key are: transport layer: ed25519 GRANDPA and consolidated reporting: BLS * block production (VRF): Ristretto 2.2 Proof-of-Stake In order to keep certain parties accountable for ensuring various properties listed below we make sure to be able to punish these participants by taking away some of their funds (Proof-of-Stake). The primary nodes running the network are the validators. To ensure a large set of participants is able to contribute to the security of the network we introduce a Nominated Proof of Stake scheme (NPoS). This scheme allows participants which do not wish to run nodes to be able to help with the validator selection. The current method used to distribute that stake is the Sequential Phragm\u00e9n Method . For Polkadot use Phragm\u00e9n's method as a fallback, but allow for better solutions to be submitted. As an edge case, if no good solution is submitted, run the slow heuristic which provides a 2-approximation (TODO: publish). Judging NPoS solutions: Check if a submitted solution is locally optimal in the sense of a certain local search procedure. Locally optimal solutions have a fairness property. Thus we only accept solutions that are fair (TODO: publish). Among the submissions that observe the first property about fairness, select the one that maximizes the minimum stake of any selected validator. This ensures maximum security threshold for each parachain validator group. A comprehensive list of misbehaviours that have to be penalized can be found in the sanctioning sheet . 2.3 Why not use different sets for different tasks? Use the same validator set for BABE as for GRANDPA as to avoid paying more in total for block production + finality. 3. Ensuring state transition properties 3.1 Usefulness Each state transition should bring some utility to the system participants. In order to ensure that this is the case: state machines should be useful to participants state transitions processed by these state machines reflect well the state transition needs of participants. To ensure that the state machines are useful we should ensure that there is a mechansim that enables participants to decide what state machines should be included and how they should change to reflect participant needs. This mechanism is the Polkadot governance scheme . To ensure that useful state transitions are processed by those state machines, we will want to ensure that useful transactions get included in Polkadot blocks. Polkadot will have a transaction fee mechanism on the relay chain to ensure that transactions issued by parties willing to pay a reasonable price for them are included. There will also be a certain portion of each block that is dedicated to certain high-priority transactions, such as misbehaviour reporting. The usefulness of the parachain state transitions has to be ensured by the state transition function of a given chain. 3.2 Validity The notion of validity in Polkadot is determined by a state transition validation function (STVF). Each chain in the ecosystem has to have one implemented. In order for all nodes to be able to run this function it is being distributed as deterministic WebAssembly (Wasm) code which can be executed by the Polkadot Runtime Environment. The blocks are produced by parachain collators, then they get validated using the STVF by the subset of validators responsible for the given parachain to finally get included in the Polkadot Relay Chain. During this process validators, parachain collators and other parties are free to challenge claims of validity to trigger additional check, these parties are referred to as fishermen. Read here about parachain validity . 3.3 Canonicality Canonicality of the Polkadot network state machines is achieved via a combination of a block production mechanism with eventual probabilistic canonicality (BABE scheme) and GRANDPA finality gadget . This approach allows for block production (thus transaction confirmations) to be fast, while allowing for as fast as possible economic finality with compact proofs. 3.4 Availability In order for the critical data from all chains to remain reachable by users and subsequent block producers, Polkadot makes use of an erasure coding based availability scheme . 4. Ensuring reliable messaging between state machines Besides ensuring all the above properties for all parachain, a crucial element of Polkadot is that these state machines are able to affect each others state transitions. This is done via the Inter-Chain Message Passing (ICMP) scheme . 5. Keeping resource usage under control 5.1 Reasonable size To ensure that the state transitions can be processed and stored by the network their size has to be reasonable. Mechanisms such as transaction fees and block limits are there to limit the storage size and computation required for each block. 5.2 Light client The protocol is being designed with light client support in mind with existing Substrate implementation supporting one. 6. Desired qualities Minimal: Polkadot should have as little functionality as possible. Simple: No additional complexity should be present in the base protocol. General: Polkadot can be optimized through making the model into which extensions fit as abstract as possible. Robust: Polkadot should provide a fundamentally stable base-layer.","title":"Home"},{"location":"#polkadot","text":"DRAFT This article summarizes the research efforts relevant to the Polkadot project, for other information regarding the project please refer to the wiki page . Polkadot speaking in abstract terms provides a number of connected canonical state machines. Connected means that a state transition of one machine can affect a transition of another machine. The state machines are canonical, since they transition in a globally consistent manner. We would also like to enable adding, removing and changing of the state machines as the time goes on. This will be the role of the governance process. The research focuses on how to enable having such publicly available system in the face of possible adversarial conditions. The public can use the system by interacting with state machines that they are interested in via the internet. Each state machine can provide different functionalities and behave in different ways (have a different state and state transition scheme). So let us start with abstract state machines. A state machine has a certain state type and state transition type. As the time goes on, state transitions occur. The data that determines the state transitions is structured as bundles of transactions - individual small state transitions triggered by the users of the system. Each bundle is called a block. In order to achieve its properties, ensures that those blocks are hash connected forming joint data structure.","title":"Polkadot"},{"location":"#1-specification-of-the-polkadot-runtime-environment","text":"We are working on a implementation level specification of the protocol here .","title":"1. Specification of the Polkadot Runtime Environment"},{"location":"#2-identifying-participants-to-run-the-network","text":"","title":"2. Identifying participants to run the network"},{"location":"#21-keys","text":"To identify unique individual participants that will perform duties on the network we use public key cryptography. You can read more about our approach here and see the particular crypto for the first implementation in the Schnorrkel repo . Validator keys indicated by the staking key are: transport layer: ed25519 GRANDPA and consolidated reporting: BLS * block production (VRF): Ristretto","title":"2.1 Keys"},{"location":"#22-proof-of-stake","text":"In order to keep certain parties accountable for ensuring various properties listed below we make sure to be able to punish these participants by taking away some of their funds (Proof-of-Stake). The primary nodes running the network are the validators. To ensure a large set of participants is able to contribute to the security of the network we introduce a Nominated Proof of Stake scheme (NPoS). This scheme allows participants which do not wish to run nodes to be able to help with the validator selection. The current method used to distribute that stake is the Sequential Phragm\u00e9n Method . For Polkadot use Phragm\u00e9n's method as a fallback, but allow for better solutions to be submitted. As an edge case, if no good solution is submitted, run the slow heuristic which provides a 2-approximation (TODO: publish). Judging NPoS solutions: Check if a submitted solution is locally optimal in the sense of a certain local search procedure. Locally optimal solutions have a fairness property. Thus we only accept solutions that are fair (TODO: publish). Among the submissions that observe the first property about fairness, select the one that maximizes the minimum stake of any selected validator. This ensures maximum security threshold for each parachain validator group. A comprehensive list of misbehaviours that have to be penalized can be found in the sanctioning sheet .","title":"2.2 Proof-of-Stake"},{"location":"#23-why-not-use-different-sets-for-different-tasks","text":"Use the same validator set for BABE as for GRANDPA as to avoid paying more in total for block production + finality.","title":"2.3 Why not use different sets for different tasks?"},{"location":"#3-ensuring-state-transition-properties","text":"","title":"3. Ensuring state transition properties"},{"location":"#31-usefulness","text":"Each state transition should bring some utility to the system participants. In order to ensure that this is the case: state machines should be useful to participants state transitions processed by these state machines reflect well the state transition needs of participants. To ensure that the state machines are useful we should ensure that there is a mechansim that enables participants to decide what state machines should be included and how they should change to reflect participant needs. This mechanism is the Polkadot governance scheme . To ensure that useful state transitions are processed by those state machines, we will want to ensure that useful transactions get included in Polkadot blocks. Polkadot will have a transaction fee mechanism on the relay chain to ensure that transactions issued by parties willing to pay a reasonable price for them are included. There will also be a certain portion of each block that is dedicated to certain high-priority transactions, such as misbehaviour reporting. The usefulness of the parachain state transitions has to be ensured by the state transition function of a given chain.","title":"3.1 Usefulness"},{"location":"#32-validity","text":"The notion of validity in Polkadot is determined by a state transition validation function (STVF). Each chain in the ecosystem has to have one implemented. In order for all nodes to be able to run this function it is being distributed as deterministic WebAssembly (Wasm) code which can be executed by the Polkadot Runtime Environment. The blocks are produced by parachain collators, then they get validated using the STVF by the subset of validators responsible for the given parachain to finally get included in the Polkadot Relay Chain. During this process validators, parachain collators and other parties are free to challenge claims of validity to trigger additional check, these parties are referred to as fishermen. Read here about parachain validity .","title":"3.2 Validity"},{"location":"#33-canonicality","text":"Canonicality of the Polkadot network state machines is achieved via a combination of a block production mechanism with eventual probabilistic canonicality (BABE scheme) and GRANDPA finality gadget . This approach allows for block production (thus transaction confirmations) to be fast, while allowing for as fast as possible economic finality with compact proofs.","title":"3.3 Canonicality"},{"location":"#34-availability","text":"In order for the critical data from all chains to remain reachable by users and subsequent block producers, Polkadot makes use of an erasure coding based availability scheme .","title":"3.4 Availability"},{"location":"#4-ensuring-reliable-messaging-between-state-machines","text":"Besides ensuring all the above properties for all parachain, a crucial element of Polkadot is that these state machines are able to affect each others state transitions. This is done via the Inter-Chain Message Passing (ICMP) scheme .","title":"4. Ensuring reliable messaging between state machines"},{"location":"#5-keeping-resource-usage-under-control","text":"","title":"5. Keeping resource usage under control"},{"location":"#51-reasonable-size","text":"To ensure that the state transitions can be processed and stored by the network their size has to be reasonable. Mechanisms such as transaction fees and block limits are there to limit the storage size and computation required for each block.","title":"5.1 Reasonable size"},{"location":"#52-light-client","text":"The protocol is being designed with light client support in mind with existing Substrate implementation supporting one.","title":"5.2 Light client"},{"location":"#6-desired-qualities","text":"Minimal: Polkadot should have as little functionality as possible. Simple: No additional complexity should be present in the base protocol. General: Polkadot can be optimized through making the model into which extensions fit as abstract as possible. Robust: Polkadot should provide a fundamentally stable base-layer.","title":"6. Desired qualities"},{"location":"ICMP/","text":"ICMP Scheme Motivation We want to enable inter-chain messageing among parachains. We want guarantee that when we send a block we are sure that we have received all the previous messages. Moreover, we want to put a limit on the size of incoming messages to avoid overflowing. Inter-chain messaging scheme assumptions We assume all parachains have an internal input and output queue. Moreover, there is a fixed amount of data each parachain can send to another parachain. To avoid parachains being flooded and over loaded, a parachain can block messages coming from other parachains. The bridges are notified of this blocking and convey it to the users on the corresponding parachain. PoV Block is execution proof witness data. This might include: Output Messages (as message bundle hash preimages) External data (\u201ctransactions\u201d, \u201cextrinsics\u201d or any other data extrinsic to Polkadot) Witness (cryptogrpahic proof statements of data validity) New header (probably only the case for verification-only STFs like zkSTARKs or whatever) Steps (Order) of Receiving and Sending Messages Belonging to a Parachain Blob: The collator needs to first check whether he has any incoming messages (which he might not have) Collator runs relay chain light client to determine relay chain header Collator tracks parachain header and that parachain\u2019s input queue as a set of hashes of message-bundles (which requires tracking all parachain\u2019s output queues since the last time the parachain accepted input) Collator queries parachain validators or collators or availability guarantors (based on relay chain data) in order to get actual message data from bundles Collator has a transaction pool The collator creates block candidate based on this transaction pool, previous header, and any other data (e.g. inherents), and incoming messages Collator builds a PoV block candidate for this parachain on the data from 3, this includes outgoing messages Collator distributes it to all parachain validators Parachain validator produces attestations on some subset of valid PoV candidates that it receives and redistributes to other parachain validators and block author Block author selects a set of PoV candidates, at most one for each parachain, which are fully attested by their group Block author distributes the relay chain block candidate to other validators Compact Routing Proofs It may be possible to use some sort of proof-of-knowledge to prove that the output queues have been routed to the correct input queues correctly, taking into account temporarily-offline parachains.","title":"ICMP Scheme"},{"location":"ICMP/#icmp-scheme","text":"","title":"ICMP Scheme"},{"location":"ICMP/#motivation","text":"We want to enable inter-chain messageing among parachains. We want guarantee that when we send a block we are sure that we have received all the previous messages. Moreover, we want to put a limit on the size of incoming messages to avoid overflowing.","title":"Motivation"},{"location":"ICMP/#inter-chain-messaging-scheme-assumptions","text":"We assume all parachains have an internal input and output queue. Moreover, there is a fixed amount of data each parachain can send to another parachain. To avoid parachains being flooded and over loaded, a parachain can block messages coming from other parachains. The bridges are notified of this blocking and convey it to the users on the corresponding parachain. PoV Block is execution proof witness data. This might include: Output Messages (as message bundle hash preimages) External data (\u201ctransactions\u201d, \u201cextrinsics\u201d or any other data extrinsic to Polkadot) Witness (cryptogrpahic proof statements of data validity) New header (probably only the case for verification-only STFs like zkSTARKs or whatever)","title":"Inter-chain messaging scheme assumptions"},{"location":"ICMP/#steps-order-of-receiving-and-sending-messages-belonging-to-a-parachain-blob","text":"The collator needs to first check whether he has any incoming messages (which he might not have) Collator runs relay chain light client to determine relay chain header Collator tracks parachain header and that parachain\u2019s input queue as a set of hashes of message-bundles (which requires tracking all parachain\u2019s output queues since the last time the parachain accepted input) Collator queries parachain validators or collators or availability guarantors (based on relay chain data) in order to get actual message data from bundles Collator has a transaction pool The collator creates block candidate based on this transaction pool, previous header, and any other data (e.g. inherents), and incoming messages Collator builds a PoV block candidate for this parachain on the data from 3, this includes outgoing messages Collator distributes it to all parachain validators Parachain validator produces attestations on some subset of valid PoV candidates that it receives and redistributes to other parachain validators and block author Block author selects a set of PoV candidates, at most one for each parachain, which are fully attested by their group Block author distributes the relay chain block candidate to other validators","title":"Steps (Order) of Receiving and Sending Messages Belonging to a Parachain Blob:"},{"location":"ICMP/#compact-routing-proofs","text":"It may be possible to use some sort of proof-of-knowledge to prove that the output queues have been routed to the correct input queues correctly, taking into account temporarily-offline parachains.","title":"Compact Routing Proofs"},{"location":"availability/","text":"Availability Motivation In Polkadot parachain collators are responsible for creating parachain blocks and sending them to parachain validators. These parachain validators must validate the block and submit summaries (called headers) to all validators of the relay chain. One of these validators is going to add this parachain header in form of a relay chain block to the relay chain. Parachain fishermen verify the validation process carried out by the parachain validators. Definition : Let us define a parachain blob as a tuple containing a light client proof of validity (PoV) for a parachain block, the parachain block itself, and the outgoing messages from that parachain. Note that we are not trusting the parachain collators or validators necessarily, but instead we rely on a number of light clients called fishermen. Fishermen are responsibile to check the validation process carried out by parachain validators. However, fishermen do not have access to all the parachain blobs since they are not full nodes of the parachain necessarily. Hence, if parachain blobs are not available fishermen would not be able to detect faulty proofs and raise any concern. Hence, if dishonest parachain validators collude with collators and create parachain headers for non-existing blobs, other relay chain blocks might be built upon non-existing parachain blobs. Therefore, once a block is created it is important that the parachain blob is available for a while. The naive solution for this would be broadcasting/gossip the parachain blobs to all, which is not a feasible option because the parachain blobs are big. We want to find an efficient solution to ensure parachain blobs from any recently created parachain block are available. Availability via Erasure Coding Let us assume we have n=3f+1 validators and at least n-f of those are honest and online. Availability Protocol A collator sends a parachain block, its outgoing messages and a light-client proof of correctness of these to parachain validators (a parachain blob) to the parachain validators Once the parachain validators have validated it, they create an erasure coded version with an optimal (n,k) block code of this blob, where k=f+1 . They also calculate a Merkle tree for this erasure coded version and add the Merkle root to the things they sign about the block for inclusion on the relay chain. The parachain validators send out these pieces along with a Merkle proof to all validators The parachain block header gets included on the relay chain If a validator has not received an erasure coded piece for every parachain blob that has a header in a relay chain block it request it. The piece they should ask for is different for each validator. Along with the piece, the parachain validator needs to provide the Merkle proof that it comes from the Merkle root on the relay chain. Validators only prevote in Grandpa for a (descendant of a) relay chain block if they have all these erasure coded pieces. They only build on blocks if they have just seen it very recently (e.g., last block) or they have all the pieces. We do not want to build on a block that has an unavailable ancestor block. The request for missing erasure coded pieces is first sent to the rest of the parachain validators and then the other full nodes of the parachain. If full nodes of the parachain don't have the parachain block available, a randomly selected validator (called assigned validator) asks all validators for their erasure coded piece of the block. When he receives k=f+1 pieces, the assigned validator attempts to reconstruct it. If a parachain fisherman publishes a proof that the block is invalid or if an intermediate validator or the validator holding the erasure coded piece refuse to hand over the piece we slash them. Moreover, if the assigned validator publishes that f+1 pieces cannot be decoded into a blob, then we slash the parachain validators and declare that relay chain block invalid. To agree on non-availability we carry out an attestation game described below. We require any one of the validators to start the attestion game by claiming that their erasure coded piece of the blob is unavailable. Now the other validators need only ask for the pieces that are claimed to be unavailable, rather than the whole blob. The initiator of the attestion game should already have asked full nodes of the parachain if the parachain validators disappeared for their piece. The idea here is that we do not finalise a block until sometime after f+1 honest validators prevote for it. But if that's the case, then 6 should succeed, which means that we only finalise available blocks. If 7 happens fast enough, then we only finalise valid and available blocks. As before, we'll need to plan for when we finalise an invalid block. The Merkle root commitment means that all parachain validators who signed off on the blob must provide the same erasure coded version. It also means that the erasure code only needs to deal with missing, rather than corrupted pieces, even if one of the guys is Byzantine, because any piece with a valid Merkle proof is the one that all the guys committed to. Now we know that if we see f+1 pieces with proof that they came from the same Merkle root in the block header, that if they don't assemble to something then all the parachain validators who signed the block header did so knowing that it didn't contain the Merkle root of a valid erasure code. So if we cannot reconstruct the blob from f+1 pieces, we can slash everyone who signed off on the Merkle root. Note: We are not going to do the 2D Reed-Solomon approach from https://arxiv.org/abs/1809.09044. If we did, it would give us smaller proofs of non-decodability. This is only worth it if proofs of invalidity of parachain blocks are likely to be smaller than the blocks themselves. So we will stick to 1D codes here. Since we know exactly how many validators we have, we can do a deterministic scheme. Attestation Game: Agreeing on non-availability How do we agree that a piece of the erasure code of a parachain blob is not available? We can have some sort of a petition (set of attestations) that can be triggered by any one of the validators for an erasure coded piece of a parachain blob when its header is on the relay chain and when they cannot retrieve the piece otherwise. This petition is broadcasted or sent around to all validators who confirm that the piece is not available and sign the petition. Once the petition is confirmed/signed by \\frac{2}{3} of the validators the collators of that parachain and the parachain validators are going to be slashed. We only need the attestation game for liveness, and we might not even need it then, so it doesn't have to be fast. The issue is that if f honest validators have a piece then that is not enough to reconstruct the blob, but as far as they know, they can still vote for it. If there are any Byzantine or offline validators, then this might stop us getting the n-f votes needed to finalise something else. In this case we might get two forks, one including the blob and one without it. If the one including the blob is longer, we need the attestation game for everyone to agree that it is invalid. In the case when f+1 validators are Byzantine and claim an unavailable blob is available, they can finalise it with the help of f validators, who have the only f pieces, and we have no way of uniquely attributing this fault. But this is also a problem for other schemes. (ask AL about it!) The data stored by each validator is actually smaller to the previous scheme we considered. If there are m parachains with parachain blocks of size s bytes, then each validator stores s/k bytes for each parachain and so ms/k < 3ms/n bytes total for this availability. The previous scheme called for like 10 additional validators per parachain to guarantee availabilty, which would result in 10ms/n extra bytes for availabilty per validator. How are missing parachain blob (pieces) retrieved? If a validators in not receiving an erasure coded piece of a parachain blob from a certain parachain validator after he has seen the header in the relay chain, it can request the missing piece from the remainder parachain validators. If those validators are also AWOL, then she can request ot from full nodes of the parachain. How do full nodes of the parachains (including collators) talk to all validators when the parachain validators, who should be the guys on both networks, are AWOL? We don't want people to be able to flood the network by asking for stuff (parachain blobs?). We could ask only one validator to collect all the pieces of a blob, assemble the blob, and forward it to the parachain light clients. The AWOLing validators would only be AWOL a parachain blob if they can go undetected, otherwise they will be slashed. Therefore, it should not be known to anyone who is going to be asked by parachain light clients in advance. Otherwise the adversary can corrupt/DoS that validator. We should ask a validator at random. We need to use a randomness that cannot be significantly biased by an adversary, e.g., randomness used to choose the validator that adds the next relay chain block. There is a low probability that the parachain validators, who are AWOLing the light nodes of the parachain, do know in advance whether they are going to be selected as an assigned validator who reconstructs the missing parachain blob. Moreover, they do not know if there is not any other validator who has the same output for their VRF, and that their AWOLing would go undetected. We could even XOR some randomnes of the VRFs of the last blocks for this? To make the it very difficult for adversary to bias the randomness. Backing up Note that an honest parachain validator can back up the pieces, before she sends them out to validators, at a (random or preferably trusted) full node of the parachain before sending them out to the validators. If the parachain validator is AWOL that parachain full node can distribute that piece to all full nodes of the parachain who can respond to requests from validators that are requesting missing erasure coded pieces. Current implementation in Polkodot PoC-4 Polkadot is currently using Reed-Solomon encoding of (n, f+1) over Finite Field of 2^{16} elements where n is the number of relay chain validators and f=\\lfloor\\frac{n-1}{3}\\rfloor to implement the availability scheme. Every parachain block is encoded by SCALE codec and is possibly padded to make sure that the encoded data is of even length. The block then is seen as a sequence of two-byte chunks each representing an element of GF(2^{16}) . The sequence is broken into f+1 -length subsequences where the final subsequence is also padded by 0 to make it of consistent length with the other subsequences. Each subsequence is treated as a message word and is encoded using Reed-Solomon (n,f+1) encoding into an n-tuple codeword vector whose elements are distributed between the validators. In this way, each subset of f+1 validators can reconstruct all of the f+1 subsequences and hence reconstruct the original parachain block. Notes from the ICMP workshop Let us assume an availability guarantor is a validator that has received an erasure coded piece according to the availabilty scheme. We use [n, f+1] erasure coding where n=3f+1 and is the number of pieces and corresponding AGs. We can tolerate slightly under a third of validators being malicious. Each AG will store 1/(f+1) erasure coded pieces of each blob. AG (validators that hold a erasured coded piece) are also GRANDPA validators. Terminology: AG: Availability guarantor PV: Parachain validator BA: Block author PoV: Proof of Validity (a block candidate plus all witness data in order to let a stateless (light) client execute it) RC: Relay chain Parachain validators or collator do the deterministic coding of the data (PoV ++ Outgoing Messages) and combine into a merkle tree, whose root is committed to in the PoV candidate and block BA authors a block with various attested-to candidates (see above steps) Each AG asks for their piece of the erasure coding from the PVs or the collator. In practice, the PVs may send them out pre-emptively. AGs, when acting as GRANDPA voters, do not vote for a block unless they have ensured availability of their piece. This guarantees that we never finalize anything which is unavailable. BAs, when authoring, should not build on chains where they do not have their piece of data available in each unfinalized block (perhaps excepting recent blocks). If BAs are unpredictable then this should help ensure that the longest chain tends to have available data. If BAs are not AGs they should be requesting random pieces of the coding. The difficulty lies in step 3; distributing the correct pieces to all correct AGs will be difficult and will require targeted messaging infrastructure. Fishermen requesting f+1 pieces in order to check the constructed data is valid will be similarly difficult. We may still need the attestation game for liveness. This scheme is inspired by the less GRANDPA specific Fraud Proofs paper. Unlike the fraud proofs paper, we don\u2019t currently make use of a 2-dimensional coding. The 2-dimensional coding gives small fraud proof transactions from a small fraction of the shares, while our variant requires reconstructing the whole data. It\u2019s impossible to verify the block\u2019s correctness without reconstruction though anyways. Also, the Fraud Proofs paper is randomized which unties them from our f+1 paramater. We could devise a deterministic variant of the Fraud Proofs paper. We expect these fraud proof transactions to be very unlikely (especially with rule #5). As an alternative optimization, we could employ a zero-knowledge proof that some f+1 erasure coded pieces reconstructs to something different than the attested-to data hash. With SNARKS, if the codes are instantiated with polynomials over the right field, they can be efficiently evaluated within a proof circuit.","title":"Availability"},{"location":"availability/#availability","text":"","title":"Availability"},{"location":"availability/#motivation","text":"In Polkadot parachain collators are responsible for creating parachain blocks and sending them to parachain validators. These parachain validators must validate the block and submit summaries (called headers) to all validators of the relay chain. One of these validators is going to add this parachain header in form of a relay chain block to the relay chain. Parachain fishermen verify the validation process carried out by the parachain validators. Definition : Let us define a parachain blob as a tuple containing a light client proof of validity (PoV) for a parachain block, the parachain block itself, and the outgoing messages from that parachain. Note that we are not trusting the parachain collators or validators necessarily, but instead we rely on a number of light clients called fishermen. Fishermen are responsibile to check the validation process carried out by parachain validators. However, fishermen do not have access to all the parachain blobs since they are not full nodes of the parachain necessarily. Hence, if parachain blobs are not available fishermen would not be able to detect faulty proofs and raise any concern. Hence, if dishonest parachain validators collude with collators and create parachain headers for non-existing blobs, other relay chain blocks might be built upon non-existing parachain blobs. Therefore, once a block is created it is important that the parachain blob is available for a while. The naive solution for this would be broadcasting/gossip the parachain blobs to all, which is not a feasible option because the parachain blobs are big. We want to find an efficient solution to ensure parachain blobs from any recently created parachain block are available.","title":"Motivation"},{"location":"availability/#availability-via-erasure-coding","text":"Let us assume we have n=3f+1 validators and at least n-f of those are honest and online.","title":"Availability via Erasure Coding"},{"location":"availability/#availability-protocol","text":"A collator sends a parachain block, its outgoing messages and a light-client proof of correctness of these to parachain validators (a parachain blob) to the parachain validators Once the parachain validators have validated it, they create an erasure coded version with an optimal (n,k) block code of this blob, where k=f+1 . They also calculate a Merkle tree for this erasure coded version and add the Merkle root to the things they sign about the block for inclusion on the relay chain. The parachain validators send out these pieces along with a Merkle proof to all validators The parachain block header gets included on the relay chain If a validator has not received an erasure coded piece for every parachain blob that has a header in a relay chain block it request it. The piece they should ask for is different for each validator. Along with the piece, the parachain validator needs to provide the Merkle proof that it comes from the Merkle root on the relay chain. Validators only prevote in Grandpa for a (descendant of a) relay chain block if they have all these erasure coded pieces. They only build on blocks if they have just seen it very recently (e.g., last block) or they have all the pieces. We do not want to build on a block that has an unavailable ancestor block. The request for missing erasure coded pieces is first sent to the rest of the parachain validators and then the other full nodes of the parachain. If full nodes of the parachain don't have the parachain block available, a randomly selected validator (called assigned validator) asks all validators for their erasure coded piece of the block. When he receives k=f+1 pieces, the assigned validator attempts to reconstruct it. If a parachain fisherman publishes a proof that the block is invalid or if an intermediate validator or the validator holding the erasure coded piece refuse to hand over the piece we slash them. Moreover, if the assigned validator publishes that f+1 pieces cannot be decoded into a blob, then we slash the parachain validators and declare that relay chain block invalid. To agree on non-availability we carry out an attestation game described below. We require any one of the validators to start the attestion game by claiming that their erasure coded piece of the blob is unavailable. Now the other validators need only ask for the pieces that are claimed to be unavailable, rather than the whole blob. The initiator of the attestion game should already have asked full nodes of the parachain if the parachain validators disappeared for their piece. The idea here is that we do not finalise a block until sometime after f+1 honest validators prevote for it. But if that's the case, then 6 should succeed, which means that we only finalise available blocks. If 7 happens fast enough, then we only finalise valid and available blocks. As before, we'll need to plan for when we finalise an invalid block. The Merkle root commitment means that all parachain validators who signed off on the blob must provide the same erasure coded version. It also means that the erasure code only needs to deal with missing, rather than corrupted pieces, even if one of the guys is Byzantine, because any piece with a valid Merkle proof is the one that all the guys committed to. Now we know that if we see f+1 pieces with proof that they came from the same Merkle root in the block header, that if they don't assemble to something then all the parachain validators who signed the block header did so knowing that it didn't contain the Merkle root of a valid erasure code. So if we cannot reconstruct the blob from f+1 pieces, we can slash everyone who signed off on the Merkle root. Note: We are not going to do the 2D Reed-Solomon approach from https://arxiv.org/abs/1809.09044. If we did, it would give us smaller proofs of non-decodability. This is only worth it if proofs of invalidity of parachain blocks are likely to be smaller than the blocks themselves. So we will stick to 1D codes here. Since we know exactly how many validators we have, we can do a deterministic scheme.","title":"Availability Protocol"},{"location":"availability/#attestation-game-agreeing-on-non-availability","text":"How do we agree that a piece of the erasure code of a parachain blob is not available? We can have some sort of a petition (set of attestations) that can be triggered by any one of the validators for an erasure coded piece of a parachain blob when its header is on the relay chain and when they cannot retrieve the piece otherwise. This petition is broadcasted or sent around to all validators who confirm that the piece is not available and sign the petition. Once the petition is confirmed/signed by \\frac{2}{3} of the validators the collators of that parachain and the parachain validators are going to be slashed. We only need the attestation game for liveness, and we might not even need it then, so it doesn't have to be fast. The issue is that if f honest validators have a piece then that is not enough to reconstruct the blob, but as far as they know, they can still vote for it. If there are any Byzantine or offline validators, then this might stop us getting the n-f votes needed to finalise something else. In this case we might get two forks, one including the blob and one without it. If the one including the blob is longer, we need the attestation game for everyone to agree that it is invalid. In the case when f+1 validators are Byzantine and claim an unavailable blob is available, they can finalise it with the help of f validators, who have the only f pieces, and we have no way of uniquely attributing this fault. But this is also a problem for other schemes. (ask AL about it!) The data stored by each validator is actually smaller to the previous scheme we considered. If there are m parachains with parachain blocks of size s bytes, then each validator stores s/k bytes for each parachain and so ms/k < 3ms/n bytes total for this availability. The previous scheme called for like 10 additional validators per parachain to guarantee availabilty, which would result in 10ms/n extra bytes for availabilty per validator.","title":"Attestation Game: Agreeing on non-availability"},{"location":"availability/#how-are-missing-parachain-blob-pieces-retrieved","text":"If a validators in not receiving an erasure coded piece of a parachain blob from a certain parachain validator after he has seen the header in the relay chain, it can request the missing piece from the remainder parachain validators. If those validators are also AWOL, then she can request ot from full nodes of the parachain. How do full nodes of the parachains (including collators) talk to all validators when the parachain validators, who should be the guys on both networks, are AWOL? We don't want people to be able to flood the network by asking for stuff (parachain blobs?). We could ask only one validator to collect all the pieces of a blob, assemble the blob, and forward it to the parachain light clients. The AWOLing validators would only be AWOL a parachain blob if they can go undetected, otherwise they will be slashed. Therefore, it should not be known to anyone who is going to be asked by parachain light clients in advance. Otherwise the adversary can corrupt/DoS that validator. We should ask a validator at random. We need to use a randomness that cannot be significantly biased by an adversary, e.g., randomness used to choose the validator that adds the next relay chain block. There is a low probability that the parachain validators, who are AWOLing the light nodes of the parachain, do know in advance whether they are going to be selected as an assigned validator who reconstructs the missing parachain blob. Moreover, they do not know if there is not any other validator who has the same output for their VRF, and that their AWOLing would go undetected. We could even XOR some randomnes of the VRFs of the last blocks for this? To make the it very difficult for adversary to bias the randomness.","title":"How are missing parachain blob (pieces) retrieved?"},{"location":"availability/#backing-up","text":"Note that an honest parachain validator can back up the pieces, before she sends them out to validators, at a (random or preferably trusted) full node of the parachain before sending them out to the validators. If the parachain validator is AWOL that parachain full node can distribute that piece to all full nodes of the parachain who can respond to requests from validators that are requesting missing erasure coded pieces.","title":"Backing up"},{"location":"availability/#current-implementation-in-polkodot-poc-4","text":"Polkadot is currently using Reed-Solomon encoding of (n, f+1) over Finite Field of 2^{16} elements where n is the number of relay chain validators and f=\\lfloor\\frac{n-1}{3}\\rfloor to implement the availability scheme. Every parachain block is encoded by SCALE codec and is possibly padded to make sure that the encoded data is of even length. The block then is seen as a sequence of two-byte chunks each representing an element of GF(2^{16}) . The sequence is broken into f+1 -length subsequences where the final subsequence is also padded by 0 to make it of consistent length with the other subsequences. Each subsequence is treated as a message word and is encoded using Reed-Solomon (n,f+1) encoding into an n-tuple codeword vector whose elements are distributed between the validators. In this way, each subset of f+1 validators can reconstruct all of the f+1 subsequences and hence reconstruct the original parachain block.","title":"Current implementation in Polkodot PoC-4"},{"location":"availability/#notes-from-the-icmp-workshop","text":"Let us assume an availability guarantor is a validator that has received an erasure coded piece according to the availabilty scheme. We use [n, f+1] erasure coding where n=3f+1 and is the number of pieces and corresponding AGs. We can tolerate slightly under a third of validators being malicious. Each AG will store 1/(f+1) erasure coded pieces of each blob. AG (validators that hold a erasured coded piece) are also GRANDPA validators. Terminology: AG: Availability guarantor PV: Parachain validator BA: Block author PoV: Proof of Validity (a block candidate plus all witness data in order to let a stateless (light) client execute it) RC: Relay chain Parachain validators or collator do the deterministic coding of the data (PoV ++ Outgoing Messages) and combine into a merkle tree, whose root is committed to in the PoV candidate and block BA authors a block with various attested-to candidates (see above steps) Each AG asks for their piece of the erasure coding from the PVs or the collator. In practice, the PVs may send them out pre-emptively. AGs, when acting as GRANDPA voters, do not vote for a block unless they have ensured availability of their piece. This guarantees that we never finalize anything which is unavailable. BAs, when authoring, should not build on chains where they do not have their piece of data available in each unfinalized block (perhaps excepting recent blocks). If BAs are unpredictable then this should help ensure that the longest chain tends to have available data. If BAs are not AGs they should be requesting random pieces of the coding. The difficulty lies in step 3; distributing the correct pieces to all correct AGs will be difficult and will require targeted messaging infrastructure. Fishermen requesting f+1 pieces in order to check the constructed data is valid will be similarly difficult. We may still need the attestation game for liveness. This scheme is inspired by the less GRANDPA specific Fraud Proofs paper. Unlike the fraud proofs paper, we don\u2019t currently make use of a 2-dimensional coding. The 2-dimensional coding gives small fraud proof transactions from a small fraction of the shares, while our variant requires reconstructing the whole data. It\u2019s impossible to verify the block\u2019s correctness without reconstruction though anyways. Also, the Fraud Proofs paper is randomized which unties them from our f+1 paramater. We could devise a deterministic variant of the Fraud Proofs paper. We expect these fraud proof transactions to be very unlikely (especially with rule #5). As an alternative optimization, we could employ a zero-knowledge proof that some f+1 erasure coded pieces reconstructs to something different than the attested-to data hash. With SNARKS, if the codes are instantiated with polynomials over the right field, they can be efficiently evaluated within a proof circuit.","title":"Notes from the ICMP workshop"},{"location":"validity/","text":"Parachain Validity How do we ensure that Parachain blocks whose header is included in the relay chain is valid? To ensure scalability, by default only a small fraction of relay chain validators can check the validity of each parachain block before it's header is included. On the other hand, a popular public parachain will have many full nodes, all of which will check the validity of the blocks in that parachain. So what we do is have a few validators validate a parachain candidate block and sign for its validity. Then if an invalid block is included, a wider group of actors known as fisherman can flag up that an invalid block is included in a relay chain block. If this happens, we need to agree on a chain that does not include this parachain block. Because this may be expensive for the protocol, we will slash the validators who signed for the parachain candidates validity 100% of their stake. The hope is that will make this a rare event. Parachain block inclusion protocol A parachain collator produces a PoV(proof of validity) block. The collator sends this PoV block to the parachain validators, the subset of relay chain validators who validate this chain. The parachain validators run the state transition validity function on the PoV block. If it passes, they sign the block header, which is a promise that the block is valid and that they will keep the PoV block available A relay chain block producer can include the parachain header in a relay chain block if it signed by a sufficently large majority of parachain validators. This ensures that any parachain block whose header is included in the relay chain is claimed to be valid by several staked validators. The parachain validators are themselves selected at random from the set of relay chain validators. This means that even if a few of the relay chain validators are malicious, it is unlikely that enough of them are validating the same parachain at the same time. However, since the number of parachain validators is small and we switch these sets often, the probability that the majority of parachain validators are malicious might not be negligible in this case. Fisherman The idea here is that a fisherman, someone who meets certain criteria, can publish somewhere a claim that a block is incorrect and a proof of that, which may be the entire PoV block. Then we do not buld on the relay chain block including that parachain header, even if it was finalised by GRANDPA and slash the parachain validators who signed it. There are a number of issues to designing a protocol for this.","title":"Parachain Validity"},{"location":"validity/#parachain-validity","text":"How do we ensure that Parachain blocks whose header is included in the relay chain is valid? To ensure scalability, by default only a small fraction of relay chain validators can check the validity of each parachain block before it's header is included. On the other hand, a popular public parachain will have many full nodes, all of which will check the validity of the blocks in that parachain. So what we do is have a few validators validate a parachain candidate block and sign for its validity. Then if an invalid block is included, a wider group of actors known as fisherman can flag up that an invalid block is included in a relay chain block. If this happens, we need to agree on a chain that does not include this parachain block. Because this may be expensive for the protocol, we will slash the validators who signed for the parachain candidates validity 100% of their stake. The hope is that will make this a rare event.","title":"Parachain Validity"},{"location":"validity/#parachain-block-inclusion-protocol","text":"A parachain collator produces a PoV(proof of validity) block. The collator sends this PoV block to the parachain validators, the subset of relay chain validators who validate this chain. The parachain validators run the state transition validity function on the PoV block. If it passes, they sign the block header, which is a promise that the block is valid and that they will keep the PoV block available A relay chain block producer can include the parachain header in a relay chain block if it signed by a sufficently large majority of parachain validators. This ensures that any parachain block whose header is included in the relay chain is claimed to be valid by several staked validators. The parachain validators are themselves selected at random from the set of relay chain validators. This means that even if a few of the relay chain validators are malicious, it is unlikely that enough of them are validating the same parachain at the same time. However, since the number of parachain validators is small and we switch these sets often, the probability that the majority of parachain validators are malicious might not be negligible in this case.","title":"Parachain block inclusion protocol"},{"location":"validity/#fisherman","text":"The idea here is that a fisherman, someone who meets certain criteria, can publish somewhere a claim that a block is incorrect and a proof of that, which may be the entire PoV block. Then we do not buld on the relay chain block including that parachain header, even if it was finalised by GRANDPA and slash the parachain validators who signed it. There are a number of issues to designing a protocol for this.","title":"Fisherman"},{"location":"NPoS/phragmen/","text":"Sequential Phragm\u00e9n Method - the simple version This note outlines a multiwinner election method introduced by Edvard Phragm\u00e9n in the 1890's and specified as a sequential greedy algorithm by Brill et al. (2017) , adapted to the problem of electing validators in Polkadot. In particular, we have adapted Brill et al.'s algorithm and proofs to the weighted case. We give needed notations in Section 1. In Section 2, we show that this algorithm runs in time O(m|E|) if each lookup and floating arithmetic operation is considered constant time, where $m$ is the number of elected validators, and $|E|$ is the number of edges in the graph (the sum over the nominators of the number of supported candidates). In Section 3, we also show that the elected commitee observes the property of Proportional Justified Representation (PJR), a popular axiom in the area of election theory showing that an election is \"fair\". Finally, in Section 4 we propose a post-computation that runs in time $\\tilde{O}(m|E|)$ (ignoring logarithmic terms) and computes for the elected set the precise budget distribution that maximizes the minimum stake in the elected committee. 1. Notation We follow the notation set on our hackmd note on the max-min support problem . Namely, an instance is given by a bipartite graph $(N\\cup V, E)$, where $nv\\in E$ represents the approval by nominator $n\\in E$ of candidate validator $v$, a vector of nominator budgets $b\\in \\mathbb{R}_{\\geq 0}^N$, and the number $m$ of candidate validators to be elected. We also denote by $V_n\\subseteq V$ the set of candidates supported by nominator $n$, and by $N_v\\subseteq N$ the set of nominators that support validator $v$. An election is given by the pair $(S,w)$ where $S\\subseteq V$ is a group of $m$ elected validators, and $w\\in \\mathbb{R} {\\geq 0}^E$ is a vector of edge weights where $w {nv}$ represents the precise amount of stake that nominator $n$ assigns to validator $v$. Besides non-negativity constraints, vector $w$ must observe the budget constraints: $\\sum_{v\\in V_n} w_{nv} \\leq b_n \\ \\forall n\\in N$. Remark : we do not consider validators to have their own budget. Rather, a validator $v$'s budget can be represented as an additional nominator having said budget and supporting only $v$, and having priority over other nominators to assign load to $v$ in the case that $v$ is elected. This priority can be ensured as a post-computation. 2. Algorithm As a high level intuition, we first find a candidate set $S\\subseteq V$ of size $m$, and then find the best edge weight vector $w$ for $S$. In this section we describe the first part of the computation, namely an algorithm that finds a candidate set $S$. It also computes an accompanying feasible weight vector $w$, which is not necessarily good. The post-computation for a better vector $w$ is given in Section 4. The sequential Phragm\u00e9n algorithm is described below. Set $S \\leftarrow \\emptyset, \\ l_n \\leftarrow 0 \\ \\forall n\\in N, \\ l_v \\leftarrow 0 \\ \\forall v\\in V$. For $i=1,\\cdots,m$: Update $l_v \\leftarrow \\frac{1+\\sum_{n\\in N_v} l_n\\cdot b_n}{\\sum_{n\\in N_v} b_n}$ for each $v\\in V\\setminus S$ ($l_v$ unchanged for $v\\in S$), Let $v_i\\in argmin_{v\\in V\\setminus S} l_v$ and update $S\\leftarrow S\\cup {v_i}$, For each $n\\in N_{v_i}$, store $w_{nv_i}\\leftarrow (l_{v_i} - l_n)b_n$, and update $l_n \\leftarrow l_{v_i}$ ($l_n$ unchanged for $n\\in N\\setminus N_{v_i}$), Update the weight vector $w\\leftarrow w/l_{v_m}$. Return set $S$ and edge weight vector $w$. Running time : We assume that each candidate validator has at least one supporter. Each one of the $m$ rounds performs $O(|E|)$ arithmetic operations, because each relation $nv\\in E$ is inspected at most twice per round. Hence, assuming that floating operations and table lookups take constant time, the running time of the algorithm is $O(m|E|)$. General idea : The algorithm elects validators sequentially. It executes $m$ rounds, electing a new validator $v_i$ in the $i$-th round, and adding it to set $S$. The algorithm also progressively builds an edge weight vector, defining all weights ${w_{nv_i}: \\ n\\in N_{v_i}}$ of edges incident to $v_i$ as soon as $v_i$ is elected. Finally, the weight vector $w$ is multiplied by a scalar to ensure it observes the budget constraints. The algorithm keeps track of scores over nominators and validators. For each nominator $n\\in N$, $n$'s score is the fraction of its budget $b_n$ that has been used up so far; i.e., $l_n:=\\frac{1}{b_n}\\sum_{v\\in V_n} w_{nv}$. The guiding principle of this heuristic is to try to minimize the maximum score $l_n$ over all nominators in each round . Consider round $i$: if a new validator $v_i$ is elected, we assign one unit of support to it, i.e. we define edge weights so that $\\sum_{n\\in N_{v_i} }w_{nv_i}=1$ (this choice of constant is irrevelant, and will change when vector $w$ is scaled in the last step). These edge weights are chosen so that all supporters of $v_i$ end up with the same score at the end of round $i$, i.e. for all $n'\\in N_{v_i}$: \\begin{align} l_{n'}^{new} &= \\frac{\\sum_{n\\in N_{v_i}} l_n^{new}\\cdot b_n}{\\sum_{n\\in N_{v_i}} b_n} \\\\ & = \\frac{\\sum_{n\\in N_{v_i}} (l_n^{old}\\cdot b_n +w_{nv_i})}{\\sum_{n\\in N_{v_i}} b_n} \\\\ & = \\frac{1+ \\sum_{n\\in N_{v_i}} l_n^{old}\\cdot b_n}{\\sum_{n\\in N_{v_i}} b_n} =: l_{v_i}.\\\\ \\end{align} This common nominator score is precisely our definition of validator $v_i$'s score $l_{v_i}$, and the algorithm greedily chooses the validator with smallest score in each round (breaking ties arbitrarily). Proof of correctness : It remains to show that the chosen edge weights are always non-negative, and that they observe the budget constraints after the last scaling. For this, we need the following lemma, which states that scores never decrease. Let $l_n^{(i)}$ and $l_v^{(i)}$ represent respectively that scores of nominator $n$ and validator $v$ at the end of the $i$-th round. Lemma 1 : $l_v^{(i)}\\leq l_v^{(i+1)}$ and $l_n^{(i)}\\leq l_n^{(i+1)}$ for each $n\\in N$, $v\\in S$ and $i<m$. Proof . We prove the inequalities by strong induction on $i$, where the base case $i=0$ is trivial if we set $l_v^{(0)}=l_n^{(0)}:=0$ for each $n$ and $v$. Assume now that all the proposed inequalities hold up to $i-1$. Validator inequalities: Consider a validator $v_j\\in S$. If $j\\leq i$, then the identity $l_{v_j}^{(i+1)}=l_{v_j}^i$ follows from the fact that a validator's score doesn't change anymore once it has been elected. Else, if $j>i$, l_{v_j}^{(i+1)}:=\\frac{1+\\sum_{n\\in N_{v_j} } b_n\\cdot l_n^{(i)}}{\\sum_{n\\in N_{v_j} } b_n} \\geq \\frac{1+\\sum_{n\\in N_{v_j} } b_n\\cdot l_n^{(i-1)}}{\\sum_{n\\in N_{v_j} } b_n} \\geq =:l_{v_j}^{(i)}, where we used the nominator inequalities $l_n^{(i-1)}\\leq l_n^{(i)}$ assumed by induction hypothesis. This shows the validator inequalities up to $i$. Nominator inequalities: Consider now a nominator $n$, and assume by contradiction that $l_n^{(i+1)}<l_n^{(i)}$. As $n$'s score has changed in round $i+1$, $n$ must support validator $v_{i+1}$, and so $l_n^{(i+1)}=l_{v_{i+1}}^{(i+1)}$. On the other hand, $l_n^{(i)}=l_n^{(j)} = l_{v_{j}}^{(j)}$ for some $j\\leq i$. Putting things together, l_{v_j}^{(j)} = l_n^{(i)} > l_n^{(i+1)} = l_{v_{i+1}}^{(i+1)} \\geq l_{v_{i+1}}^{(j)}, where the last inequality follows from validator inequalities up to $i$, which we just proved in the previous paragraph. We conclude that in round $j$, validator $v_{i+1}$ had a strictly smaller score than $v_j$, which contradicts the choice of $v_j$. $\\square$ It easily follows that all edge weights are non-negative. Moreover, using the definition of the nominator scores, before the final weight scaling the budget inequalities are equivalent to $l_n\\leq l_{v_m}$ for each $n\\in N$, and this inequality holds because for each $n\\in N$ there is an $i\\leq m$ such that $l_n=l_{v_i}\\leq l_{v_m}$. Section 4. Axiomatic properties In the research literature of approval-based miltiwinner elections, it is common to take an axiomatic approach and define properties of voting methods that are intuitively desirable (see our main reference Brill et al. (2017) , as well as S\u00e1nchez-Fern\u00e1ndez et al. (2018) ). These properties apply to the elected committee only, ignoring the edge weights. For example, a voting method is called house monotonic if, for any instance, the elected candidates are all still elected if the number $m$ of winners is increased. As our algorithm elects validators iteratively, it is trivially house monotonic. We focus on the property of proportional justified representation (PJR), which establishes that if a group of nominators has sufficient budget, and their preferences are sufficiently aligned, then they must be well represented in the elected committee. More formally, a voting method satifies PJR if for any instance $(N\\cup V, E, b, m)$ electing a committee $S$, and any integer $1\\leq t\\leq m$, there is no nominator subset $N'\\subseteq N$ such that $\\sum_{n \\in N'} b_n \\geq \\frac{t}{m} \\cdot \\sum_{n \\in N} b_n$, $|\\cap_{n\\in N'} V_n| \\geq t$, and * $|S\\cap (\\cup_{n\\in N'} V_n)| < t$. Brill et al (2017) proved that the proposed algorithm, sequential Phragm\u00e9n, satifies PJR, making it the first known polynomial-time method with this property. We present a proof next. Lemma 2: Sequential Phragm\u00e9n satisfies PJR. Proof: Assume the opposite, hence there is an instance $(N\\cup V, E, b, m)$ with output committe $S$, an integer $1\\leq t\\leq m$ and a nominator subset $N'\\subseteq N$ as in the definition above. For simplicity, we ignore the last scaling of the edge weight vector in the algorithm. Hence, every elected validator in $S$ receives a support of one unit, and the sum of supports over $S$ is $m$. Since we know that each budget constraint is violated by a multiplicative term of at most $l_{v_m}$ (the score of the last added validator), we obtain the bound \\begin{equation} l_{v_m}\\geq \\frac{m}{\\sum_{n\\in N} b_n}. \\end{equation} As $l_{v_m}$ is an upper bound on the nominator score $l_n$ for each $n\\in N$ (by Lemma 1), and $l_n$ is the proportion of $m$'s budget that's used, the previous inequality is tight only if $l_n = m/\\sum_{n\\in N} b_n$ for each $n\\in N$. Let $S'=S\\cap(\\cup_n\\in N') V_n$, where $|S|<=t-1$ by hypothesis. Since nominators in $N'$ only need to provide support to validators in $S'$, the sum over $N'$ of used budgets must be smaller than $|S'|$, i.e. \\sum_{n\\in N'} l_n\\cdot b_n \\leq |S'| <= t-1. By a (weighted) average argument, this implies that there is a nominator $n'\\in N'$ with score l_{n'}\\leq \\frac{\\sum_{n\\in N'} l_n\\cdot b_n}{\\sum_{n\\in N'} b_n} < \\frac{t}{\\sum_{n\\in N'} b_n} \\leq \\frac{t}{\\frac{t}{m} \\sum_{n\\in N} b_n} = \\frac{m}{ \\sum_{n\\in N} b_n}, where the last inequality is by hypothesis. This implies that the inequality $l_{v_m} > m/\\sum_{n\\in N} b_n$ is not tight. Consider now running a new round (round $m+1$) on the algorithm, and fix an unelected validator $v'\\in \\cap_{n\\in N'} V_n$ (which must exist by hypothesis). If we compute the score of $v'$ in this round, we get l_{v'} = \\frac{1+\\sum_{n\\in N_{v'} } l_n\\cdot b_n}{\\sum_{n\\in N_{v'}} b_n}\\leq \\frac{1+\\sum_{n\\in N'} l_n\\cdot b_n}{\\sum_{n\\in N'} b_n}, where we used the fact that $N'\\subseteq N_{v'}$, and that reducing the set of nominators over which the unit support for $v'$ is split can only increase the nominator scores. Using the known upper bound on the nominator, and the known lower bound on the denominator, we obtain l_{v'}\\leq \\frac{1+\\sum_{n\\in N'} l_n\\cdot b_n}{\\sum_{n\\in N'} b_n} \\leq \\frac{1 + (t-1)}{\\frac{t}{m} \\sum_{n\\in N} b_n} = \\frac{m}{\\sum_{n\\in N} b_n} < l_{v_m}. This implies that $l_{v_m} > l_{v'} \\geq l_{v_{m+1}}$, which contradicts Lemma 1. $\\square$ Section 4. Post-computation for edge weights Recall that the axiomatic properties in the previous section are independent of the edge weight vector returned by the algorithm. Hence, we can modify this vector at will while still keeping the properties. In particular, we can find the weight vector that maximizes the minimum support over the elected set $S$, up to a factor of $(1-\\varepsilon)$, in time $O(m|E|\\log(\\log (k)/\\varepsilon))$, following the algorithm described in our hackmd note on the max-support problem , with the only difference that we do not need to perform the inner loop which selects different tentative committees. Section 5. Alistair's analysis. We now describe a round of the algorithm. Let $S$ be the set of elected candidates so far. In each round we compute for every unelected candidate t_v = \\frac{1+ \\sum_{n:v \\in A_n} l_n s_n}{\\sum_{n:v \\in A_n} s_n} where $l_n = \\max_{v \\in A_n \\cap E} t_v$ and $l_n=0$ if $A_n \\cap E$ is empty. (Note that this definition is not circular since the $l_n$s only depend on the $t_v$ for elected validators.) Then we elect the candidate with the least $t_v$ and store that value. After the $m$th round, when we have elected the desired number of candidates $m$, we assign the weights of nominator $n$ with non-empty $A_n \\cap E$ to a validator $v \\in A_n \\cap E$ as follows: w_{n,v} = (s_n/t_{v_{last}}) (t_v - t_{v_{prev}}) where $v_{prev}$ is the validator in $A_n \\cap E$ that immediately precedes $v$ in order of election and $v_{last}$ is the validator in $A_n \\cap E$ that was elected last. If $v$ was elected first in $A_n \\cap E$, then take $t_{v_{prev}}=0$. It is easy to show by induction that the $t_v$ of elected candidates is increasing (but not necessarily strictly). As long as it is the existing elected candidates, then $l_n$ for every nominator and $t_v$ for unelected candidates increase with time. Since the next candidate to be elected didn't get elected in the last election, its $t_v$ was at least as high as the least elected candidate then and so is still at least as high now. Thus $t_v - t_{v_{prev}}$ terms are all non-negative and sum to $t_{v_{last}}$. Hence the $w_{n,v}$ are all non-negative and $\\sum_v w_{n,v} = s_n$ for any nominator with $A_n \\cap E$ non-empty. Incentiving the nominators to distribute the stake for more security Note that any nominator who nominates any elected validator has all their stake assigned. Nothing stops most of them just voting for one candidate, in which case all stake is assigned, but the security is low. We should try to incentivize nominators to avoid this. One way of doing that would be to compute $s_v = \\sum_n w_{n,v}$ for every elected validator, then compute, say, the ($m- \\lfloor m/3 \\rfloor$)-th highest $s_v$, $s$, then weight the nominators payments by some factor, e.g. paying nominator $n$ proportionally to \\sum_v w_{n,v} \\cdot \\min \\{2, 1+ s/s_v\\}, so any nominator has an incentive to vote for validators in the bottom $1/3$, which would increase security. We can replace $1/3$ with something smaller for Polkadot-style chains or be even more aggressive than a factor of two if necessary Implementation and time complexity Let $V$ be the total bit size of the votes, $c$ the number of candidates and $m$ as before the number of validators we elect. We claim that the running time $O(V(m+\\log \\max_n |A_n|))$ or $O(V(m+\\log c+\\log \\max_n |A_n|))$ or $O(V(m + \\log \\max_n |A_n|)\\log c)$ depending on data structure choice. We note that if candidate identifiers are of fixed length then $V \\geq (\\log_2 c) \\sum_n |A_n|$. We store the candidates state in some data structure with O(log c) addition, lookup and update and note that either a) we can make the cost of these operations linear in the candidate identifier size, in which case we get O(Vm) time, b) after preporcessing, we can store the candidates data in an array, replacing all identifier with indices in the votes, in which case we get $O(V(m+\\log c))$ time, or * c) we can just not bother with such ridiculous optimizations and get $O(Vm \\log c)$ time. After doing this we can do each of the following in one iteration through the vote list 1. compute $\\sum_{n: v \\in A_n} s_n$ for every candidate who recieves a nomination 2. compute the scores $t_v$ for every unelected candidate in any round or 3. compute $w_n,v$ for each nominator For 2, we note that we can compute $l_n$ with one iteration through the nominator's votes and $|A_n|$ candidate lookups. Then we can add $l_n s_n/(\\sum_{n: v \\in A_n} s_n)$ to each unelected validator in $A_n \\setminus E$ again in an iteration through the the nominator's votes and $|A_n|$ candidate lookups and updates. For 3, we'll need to sort $A_n \\cap E$ by election order, which will take time $O(|A_n| \\log \\max_n |A_n|)$. Then we just need to look up $t_v$ for each candidate. For the incentive thing, we'll need a pass over the votes to compute $\\sum_v w_n,v$ for each $v \\in E$, then time $O(m \\log m)$ to sort the validator list by it.","title":"Sequential Phragm\u00e9n Method - the simple version"},{"location":"NPoS/phragmen/#sequential-phragmen-method-the-simple-version","text":"This note outlines a multiwinner election method introduced by Edvard Phragm\u00e9n in the 1890's and specified as a sequential greedy algorithm by Brill et al. (2017) , adapted to the problem of electing validators in Polkadot. In particular, we have adapted Brill et al.'s algorithm and proofs to the weighted case. We give needed notations in Section 1. In Section 2, we show that this algorithm runs in time O(m|E|) if each lookup and floating arithmetic operation is considered constant time, where $m$ is the number of elected validators, and $|E|$ is the number of edges in the graph (the sum over the nominators of the number of supported candidates). In Section 3, we also show that the elected commitee observes the property of Proportional Justified Representation (PJR), a popular axiom in the area of election theory showing that an election is \"fair\". Finally, in Section 4 we propose a post-computation that runs in time $\\tilde{O}(m|E|)$ (ignoring logarithmic terms) and computes for the elected set the precise budget distribution that maximizes the minimum stake in the elected committee.","title":"Sequential Phragm\u00e9n Method - the simple version"},{"location":"NPoS/phragmen/#1-notation","text":"We follow the notation set on our hackmd note on the max-min support problem . Namely, an instance is given by a bipartite graph $(N\\cup V, E)$, where $nv\\in E$ represents the approval by nominator $n\\in E$ of candidate validator $v$, a vector of nominator budgets $b\\in \\mathbb{R}_{\\geq 0}^N$, and the number $m$ of candidate validators to be elected. We also denote by $V_n\\subseteq V$ the set of candidates supported by nominator $n$, and by $N_v\\subseteq N$ the set of nominators that support validator $v$. An election is given by the pair $(S,w)$ where $S\\subseteq V$ is a group of $m$ elected validators, and $w\\in \\mathbb{R} {\\geq 0}^E$ is a vector of edge weights where $w {nv}$ represents the precise amount of stake that nominator $n$ assigns to validator $v$. Besides non-negativity constraints, vector $w$ must observe the budget constraints: $\\sum_{v\\in V_n} w_{nv} \\leq b_n \\ \\forall n\\in N$. Remark : we do not consider validators to have their own budget. Rather, a validator $v$'s budget can be represented as an additional nominator having said budget and supporting only $v$, and having priority over other nominators to assign load to $v$ in the case that $v$ is elected. This priority can be ensured as a post-computation.","title":"1. Notation"},{"location":"NPoS/phragmen/#2-algorithm","text":"As a high level intuition, we first find a candidate set $S\\subseteq V$ of size $m$, and then find the best edge weight vector $w$ for $S$. In this section we describe the first part of the computation, namely an algorithm that finds a candidate set $S$. It also computes an accompanying feasible weight vector $w$, which is not necessarily good. The post-computation for a better vector $w$ is given in Section 4. The sequential Phragm\u00e9n algorithm is described below. Set $S \\leftarrow \\emptyset, \\ l_n \\leftarrow 0 \\ \\forall n\\in N, \\ l_v \\leftarrow 0 \\ \\forall v\\in V$. For $i=1,\\cdots,m$: Update $l_v \\leftarrow \\frac{1+\\sum_{n\\in N_v} l_n\\cdot b_n}{\\sum_{n\\in N_v} b_n}$ for each $v\\in V\\setminus S$ ($l_v$ unchanged for $v\\in S$), Let $v_i\\in argmin_{v\\in V\\setminus S} l_v$ and update $S\\leftarrow S\\cup {v_i}$, For each $n\\in N_{v_i}$, store $w_{nv_i}\\leftarrow (l_{v_i} - l_n)b_n$, and update $l_n \\leftarrow l_{v_i}$ ($l_n$ unchanged for $n\\in N\\setminus N_{v_i}$), Update the weight vector $w\\leftarrow w/l_{v_m}$. Return set $S$ and edge weight vector $w$. Running time : We assume that each candidate validator has at least one supporter. Each one of the $m$ rounds performs $O(|E|)$ arithmetic operations, because each relation $nv\\in E$ is inspected at most twice per round. Hence, assuming that floating operations and table lookups take constant time, the running time of the algorithm is $O(m|E|)$. General idea : The algorithm elects validators sequentially. It executes $m$ rounds, electing a new validator $v_i$ in the $i$-th round, and adding it to set $S$. The algorithm also progressively builds an edge weight vector, defining all weights ${w_{nv_i}: \\ n\\in N_{v_i}}$ of edges incident to $v_i$ as soon as $v_i$ is elected. Finally, the weight vector $w$ is multiplied by a scalar to ensure it observes the budget constraints. The algorithm keeps track of scores over nominators and validators. For each nominator $n\\in N$, $n$'s score is the fraction of its budget $b_n$ that has been used up so far; i.e., $l_n:=\\frac{1}{b_n}\\sum_{v\\in V_n} w_{nv}$. The guiding principle of this heuristic is to try to minimize the maximum score $l_n$ over all nominators in each round . Consider round $i$: if a new validator $v_i$ is elected, we assign one unit of support to it, i.e. we define edge weights so that $\\sum_{n\\in N_{v_i} }w_{nv_i}=1$ (this choice of constant is irrevelant, and will change when vector $w$ is scaled in the last step). These edge weights are chosen so that all supporters of $v_i$ end up with the same score at the end of round $i$, i.e. for all $n'\\in N_{v_i}$: \\begin{align} l_{n'}^{new} &= \\frac{\\sum_{n\\in N_{v_i}} l_n^{new}\\cdot b_n}{\\sum_{n\\in N_{v_i}} b_n} \\\\ & = \\frac{\\sum_{n\\in N_{v_i}} (l_n^{old}\\cdot b_n +w_{nv_i})}{\\sum_{n\\in N_{v_i}} b_n} \\\\ & = \\frac{1+ \\sum_{n\\in N_{v_i}} l_n^{old}\\cdot b_n}{\\sum_{n\\in N_{v_i}} b_n} =: l_{v_i}.\\\\ \\end{align} This common nominator score is precisely our definition of validator $v_i$'s score $l_{v_i}$, and the algorithm greedily chooses the validator with smallest score in each round (breaking ties arbitrarily). Proof of correctness : It remains to show that the chosen edge weights are always non-negative, and that they observe the budget constraints after the last scaling. For this, we need the following lemma, which states that scores never decrease. Let $l_n^{(i)}$ and $l_v^{(i)}$ represent respectively that scores of nominator $n$ and validator $v$ at the end of the $i$-th round. Lemma 1 : $l_v^{(i)}\\leq l_v^{(i+1)}$ and $l_n^{(i)}\\leq l_n^{(i+1)}$ for each $n\\in N$, $v\\in S$ and $i<m$. Proof . We prove the inequalities by strong induction on $i$, where the base case $i=0$ is trivial if we set $l_v^{(0)}=l_n^{(0)}:=0$ for each $n$ and $v$. Assume now that all the proposed inequalities hold up to $i-1$. Validator inequalities: Consider a validator $v_j\\in S$. If $j\\leq i$, then the identity $l_{v_j}^{(i+1)}=l_{v_j}^i$ follows from the fact that a validator's score doesn't change anymore once it has been elected. Else, if $j>i$, l_{v_j}^{(i+1)}:=\\frac{1+\\sum_{n\\in N_{v_j} } b_n\\cdot l_n^{(i)}}{\\sum_{n\\in N_{v_j} } b_n} \\geq \\frac{1+\\sum_{n\\in N_{v_j} } b_n\\cdot l_n^{(i-1)}}{\\sum_{n\\in N_{v_j} } b_n} \\geq =:l_{v_j}^{(i)}, where we used the nominator inequalities $l_n^{(i-1)}\\leq l_n^{(i)}$ assumed by induction hypothesis. This shows the validator inequalities up to $i$. Nominator inequalities: Consider now a nominator $n$, and assume by contradiction that $l_n^{(i+1)}<l_n^{(i)}$. As $n$'s score has changed in round $i+1$, $n$ must support validator $v_{i+1}$, and so $l_n^{(i+1)}=l_{v_{i+1}}^{(i+1)}$. On the other hand, $l_n^{(i)}=l_n^{(j)} = l_{v_{j}}^{(j)}$ for some $j\\leq i$. Putting things together, l_{v_j}^{(j)} = l_n^{(i)} > l_n^{(i+1)} = l_{v_{i+1}}^{(i+1)} \\geq l_{v_{i+1}}^{(j)}, where the last inequality follows from validator inequalities up to $i$, which we just proved in the previous paragraph. We conclude that in round $j$, validator $v_{i+1}$ had a strictly smaller score than $v_j$, which contradicts the choice of $v_j$. $\\square$ It easily follows that all edge weights are non-negative. Moreover, using the definition of the nominator scores, before the final weight scaling the budget inequalities are equivalent to $l_n\\leq l_{v_m}$ for each $n\\in N$, and this inequality holds because for each $n\\in N$ there is an $i\\leq m$ such that $l_n=l_{v_i}\\leq l_{v_m}$.","title":"2. Algorithm"},{"location":"NPoS/phragmen/#section-4-axiomatic-properties","text":"In the research literature of approval-based miltiwinner elections, it is common to take an axiomatic approach and define properties of voting methods that are intuitively desirable (see our main reference Brill et al. (2017) , as well as S\u00e1nchez-Fern\u00e1ndez et al. (2018) ). These properties apply to the elected committee only, ignoring the edge weights. For example, a voting method is called house monotonic if, for any instance, the elected candidates are all still elected if the number $m$ of winners is increased. As our algorithm elects validators iteratively, it is trivially house monotonic. We focus on the property of proportional justified representation (PJR), which establishes that if a group of nominators has sufficient budget, and their preferences are sufficiently aligned, then they must be well represented in the elected committee. More formally, a voting method satifies PJR if for any instance $(N\\cup V, E, b, m)$ electing a committee $S$, and any integer $1\\leq t\\leq m$, there is no nominator subset $N'\\subseteq N$ such that $\\sum_{n \\in N'} b_n \\geq \\frac{t}{m} \\cdot \\sum_{n \\in N} b_n$, $|\\cap_{n\\in N'} V_n| \\geq t$, and * $|S\\cap (\\cup_{n\\in N'} V_n)| < t$. Brill et al (2017) proved that the proposed algorithm, sequential Phragm\u00e9n, satifies PJR, making it the first known polynomial-time method with this property. We present a proof next. Lemma 2: Sequential Phragm\u00e9n satisfies PJR. Proof: Assume the opposite, hence there is an instance $(N\\cup V, E, b, m)$ with output committe $S$, an integer $1\\leq t\\leq m$ and a nominator subset $N'\\subseteq N$ as in the definition above. For simplicity, we ignore the last scaling of the edge weight vector in the algorithm. Hence, every elected validator in $S$ receives a support of one unit, and the sum of supports over $S$ is $m$. Since we know that each budget constraint is violated by a multiplicative term of at most $l_{v_m}$ (the score of the last added validator), we obtain the bound \\begin{equation} l_{v_m}\\geq \\frac{m}{\\sum_{n\\in N} b_n}. \\end{equation} As $l_{v_m}$ is an upper bound on the nominator score $l_n$ for each $n\\in N$ (by Lemma 1), and $l_n$ is the proportion of $m$'s budget that's used, the previous inequality is tight only if $l_n = m/\\sum_{n\\in N} b_n$ for each $n\\in N$. Let $S'=S\\cap(\\cup_n\\in N') V_n$, where $|S|<=t-1$ by hypothesis. Since nominators in $N'$ only need to provide support to validators in $S'$, the sum over $N'$ of used budgets must be smaller than $|S'|$, i.e. \\sum_{n\\in N'} l_n\\cdot b_n \\leq |S'| <= t-1. By a (weighted) average argument, this implies that there is a nominator $n'\\in N'$ with score l_{n'}\\leq \\frac{\\sum_{n\\in N'} l_n\\cdot b_n}{\\sum_{n\\in N'} b_n} < \\frac{t}{\\sum_{n\\in N'} b_n} \\leq \\frac{t}{\\frac{t}{m} \\sum_{n\\in N} b_n} = \\frac{m}{ \\sum_{n\\in N} b_n}, where the last inequality is by hypothesis. This implies that the inequality $l_{v_m} > m/\\sum_{n\\in N} b_n$ is not tight. Consider now running a new round (round $m+1$) on the algorithm, and fix an unelected validator $v'\\in \\cap_{n\\in N'} V_n$ (which must exist by hypothesis). If we compute the score of $v'$ in this round, we get l_{v'} = \\frac{1+\\sum_{n\\in N_{v'} } l_n\\cdot b_n}{\\sum_{n\\in N_{v'}} b_n}\\leq \\frac{1+\\sum_{n\\in N'} l_n\\cdot b_n}{\\sum_{n\\in N'} b_n}, where we used the fact that $N'\\subseteq N_{v'}$, and that reducing the set of nominators over which the unit support for $v'$ is split can only increase the nominator scores. Using the known upper bound on the nominator, and the known lower bound on the denominator, we obtain l_{v'}\\leq \\frac{1+\\sum_{n\\in N'} l_n\\cdot b_n}{\\sum_{n\\in N'} b_n} \\leq \\frac{1 + (t-1)}{\\frac{t}{m} \\sum_{n\\in N} b_n} = \\frac{m}{\\sum_{n\\in N} b_n} < l_{v_m}. This implies that $l_{v_m} > l_{v'} \\geq l_{v_{m+1}}$, which contradicts Lemma 1. $\\square$","title":"Section 4. Axiomatic properties"},{"location":"NPoS/phragmen/#section-4-post-computation-for-edge-weights","text":"Recall that the axiomatic properties in the previous section are independent of the edge weight vector returned by the algorithm. Hence, we can modify this vector at will while still keeping the properties. In particular, we can find the weight vector that maximizes the minimum support over the elected set $S$, up to a factor of $(1-\\varepsilon)$, in time $O(m|E|\\log(\\log (k)/\\varepsilon))$, following the algorithm described in our hackmd note on the max-support problem , with the only difference that we do not need to perform the inner loop which selects different tentative committees.","title":"Section 4. Post-computation for edge weights"},{"location":"NPoS/phragmen/#section-5-alistairs-analysis","text":"We now describe a round of the algorithm. Let $S$ be the set of elected candidates so far. In each round we compute for every unelected candidate t_v = \\frac{1+ \\sum_{n:v \\in A_n} l_n s_n}{\\sum_{n:v \\in A_n} s_n} where $l_n = \\max_{v \\in A_n \\cap E} t_v$ and $l_n=0$ if $A_n \\cap E$ is empty. (Note that this definition is not circular since the $l_n$s only depend on the $t_v$ for elected validators.) Then we elect the candidate with the least $t_v$ and store that value. After the $m$th round, when we have elected the desired number of candidates $m$, we assign the weights of nominator $n$ with non-empty $A_n \\cap E$ to a validator $v \\in A_n \\cap E$ as follows: w_{n,v} = (s_n/t_{v_{last}}) (t_v - t_{v_{prev}}) where $v_{prev}$ is the validator in $A_n \\cap E$ that immediately precedes $v$ in order of election and $v_{last}$ is the validator in $A_n \\cap E$ that was elected last. If $v$ was elected first in $A_n \\cap E$, then take $t_{v_{prev}}=0$. It is easy to show by induction that the $t_v$ of elected candidates is increasing (but not necessarily strictly). As long as it is the existing elected candidates, then $l_n$ for every nominator and $t_v$ for unelected candidates increase with time. Since the next candidate to be elected didn't get elected in the last election, its $t_v$ was at least as high as the least elected candidate then and so is still at least as high now. Thus $t_v - t_{v_{prev}}$ terms are all non-negative and sum to $t_{v_{last}}$. Hence the $w_{n,v}$ are all non-negative and $\\sum_v w_{n,v} = s_n$ for any nominator with $A_n \\cap E$ non-empty.","title":"Section 5. Alistair's analysis."},{"location":"NPoS/phragmen/#incentiving-the-nominators-to-distribute-the-stake-for-more-security","text":"Note that any nominator who nominates any elected validator has all their stake assigned. Nothing stops most of them just voting for one candidate, in which case all stake is assigned, but the security is low. We should try to incentivize nominators to avoid this. One way of doing that would be to compute $s_v = \\sum_n w_{n,v}$ for every elected validator, then compute, say, the ($m- \\lfloor m/3 \\rfloor$)-th highest $s_v$, $s$, then weight the nominators payments by some factor, e.g. paying nominator $n$ proportionally to \\sum_v w_{n,v} \\cdot \\min \\{2, 1+ s/s_v\\}, so any nominator has an incentive to vote for validators in the bottom $1/3$, which would increase security. We can replace $1/3$ with something smaller for Polkadot-style chains or be even more aggressive than a factor of two if necessary","title":"Incentiving the nominators to distribute the stake for more security"},{"location":"NPoS/phragmen/#implementation-and-time-complexity","text":"Let $V$ be the total bit size of the votes, $c$ the number of candidates and $m$ as before the number of validators we elect. We claim that the running time $O(V(m+\\log \\max_n |A_n|))$ or $O(V(m+\\log c+\\log \\max_n |A_n|))$ or $O(V(m + \\log \\max_n |A_n|)\\log c)$ depending on data structure choice. We note that if candidate identifiers are of fixed length then $V \\geq (\\log_2 c) \\sum_n |A_n|$. We store the candidates state in some data structure with O(log c) addition, lookup and update and note that either a) we can make the cost of these operations linear in the candidate identifier size, in which case we get O(Vm) time, b) after preporcessing, we can store the candidates data in an array, replacing all identifier with indices in the votes, in which case we get $O(V(m+\\log c))$ time, or * c) we can just not bother with such ridiculous optimizations and get $O(Vm \\log c)$ time. After doing this we can do each of the following in one iteration through the vote list 1. compute $\\sum_{n: v \\in A_n} s_n$ for every candidate who recieves a nomination 2. compute the scores $t_v$ for every unelected candidate in any round or 3. compute $w_n,v$ for each nominator For 2, we note that we can compute $l_n$ with one iteration through the nominator's votes and $|A_n|$ candidate lookups. Then we can add $l_n s_n/(\\sum_{n: v \\in A_n} s_n)$ to each unelected validator in $A_n \\setminus E$ again in an iteration through the the nominator's votes and $|A_n|$ candidate lookups and updates. For 3, we'll need to sort $A_n \\cap E$ by election order, which will take time $O(|A_n| \\log \\max_n |A_n|)$. Then we just need to look up $t_v$ for each candidate. For the incentive thing, we'll need a pass over the votes to compute $\\sum_v w_n,v$ for each $v \\in E$, then time $O(m \\log m)$ to sort the validator list by it.","title":"Implementation and time complexity"},{"location":"bridges/nope/swap/","text":"UPDATE: This discussion of swaps implemented with ZKCP might prove relevant to building bridges, but a bridge itself exists to support clients who cannot track chains. We envision a bridge that manages security for atomic swaps by observing the bitcoin blockchain, but does not itself hold bitcoins, and only holds dots as stake. Aside from the bridge nodes and polkadot infrastructure, we have two customers, Paula who trade dots and Brit who trades bitcoin, although atomicity could generalise either to a group. I'll give a handy wavy sketch that should be informed by going over ZKCP literature: Step 0. Paula's offer of Dot and Brit's offer of BTC get matched via some trading network, placing them into communication. I donno if MPC helps here. Step 1. Paula and Brit negotiate their transaction: - Paula provides Brit with her BTC wallet hash and her bridge supported test for considering the transaction settled on bitcoin. - If Brits consider the test reasonable, then Brit secretly crafts but does not publish her BTC payment x to Paula, and gives Paula the hash of x. Step 2. Paula submits a bridge parachain transaction y to Brit that registers the trade with the bridge and time locks the funds she wishes to trade. Registration reveals the test, the Dot and BTC amounts, Paula's BTC wallet hash, and the BTC transaction hash. Step 3. After the parachain transaction is finalised, Brit submits her BTC transaction x, or maybe gives it to Paula so she can submit it. If Brit does not do so fast enough then Paula's time lock expires. Step 4. The bridges publish and finalize a transaction signing the release of funds from y to Brit, after recognising that x matches its hash commitment, sends the correct BTC amount to Paula's BTC hash, and has persisted long enough to past the test. Importantly, we use ordinary parachain finalization logic for the \"threshold\" signature by the bridges in steps 3 and 4, which should be a simple aggregate signature, not a true threshold signature, which anonymises the signers, so that the individual bridges can be held accountable. We could improve perceived latency for Brit, and maybe simplify client code, by having Brit threshold encrypt x to the bridges in step 2, so the bridges carry out step 3 after finalising. We expect polkadot to be fast though, so imho Brit should prefer the latency and simpler security model above. Also I'd expect the above scheme simplifies bridge code far more than alternative schemes simplify client code. In this design, Brit needs to trust the bridges will eventually unlock Paula's transaction y and Paula needs to trust that the bridges will wait until the test for Brit's transaction x checks out before unlicking her transaction y. Attack 1. Bridges screw Brit. At minimum Brit can appeal to governance to manually slash the bridges. We could automatically slash the bridges based on evidence from bitcoin, but we cannot assess network conditions automatically and someone must assess this evidence, and worse hostil bitcoin miners could attack bridges via slashing. We could make Brit produce a zero-knowledge or WI proof that her transaction worse as desired, but either bridges must still assess bitcoin, or else \"dumb\" tests much be hard wired into the parachain, like show 12 bitcoin blocks with however many zeros, which sounds game-able by bitcoin miners. Attack 2. Bridges screw Paula. We could make Brit actually publish a time locked contract x' on bitcoin that requires revealing a hash x'' and the bridges merely wait until passing the bitcoin test before revealing x''. We still have a fair exchange problem in how long x' and y lock funds, which malicious bridges may exploit. If Brit submits a time locked transaction this way, then ultimately the swap requires two BTC transactions, which sounds expensive, and may add latency for Paula.","title":"Swap"},{"location":"crypto-old/NetRNGs/","text":"Network randomness schemes These are our reviews of collaborative random number generators (RNGs) or other networked randomness schemes Collaborative PRNGs There are several true collaborative PRNG schemes in which all participants obtain the same output and can then run arbitrary randomised algorithms with identical seeds and results. As a result, we can employ more randomised algorithms that produce exact outputs like permutations of nodes, or require more complexity or tweaking. Verifiable Delay Functions (VDFs) VDFs employ a slow non-parallelizable computation and provide a PRNG with a proof that some time period elapsed between when seeds were available and when the output was available. Right now, the best ideas for this want a group of unknown order, like maybe the class group of an imaginary quadratic number field. https://crypto.stanford.edu/~dabo/pubs/papers/VDFsurvey.pdf Justin Drake with Etherium research is very interested in VDFs because VDFs provide a collaborative PRNG in which one honest party suffices to produce a secure random number. We currently fear that VDFs deployment strategies have an excessively small security margin. An attacker might for example compute the VDF 10x faster to gains the random number early or even bias it. ASICS might be achieve this. At the extreme, superconducting computing can achieve speeds of 100 GHz with circuits consisting of 10^5 Josephson junctions. You\u2019d need a rather large time window to use a VDFs, and you\u2019d need an even larger time window to handle the highly optimised VDF node equivocating. DFinity style VSS + VRF We might not fully understand the VRF's roll here because the public key correctness ultimately depends on the VSS based DKG and this depends upon validator nodes raising accusations. We believe they do this so that one DKG yields numerous VRF outputs, maybe because the DKG is considerably more expensive, or not amenable to some optimisations. Almost all the pure cryptography implementation work consists of following the DKG design in Secure Distributed Key Generation for Discrete-Log Based Cryptosystems by Rosario Gennaro, Stanisl\udbff\udc51aw Jarecki, Hugo Krawczyk, and Tal Rabin. There is no actual production of a BLS signature produced there however, so one should also look over section 7 of DFINITY Technology Overview Series Consensus System (Rev. 1) by Timo Hanke, Mahnush Movahedi and Dominic Williams. Schoenmakers PVSS ala EPFL \"A publicly verifiable secret sharing (PVSS) scheme is a verifiable secret sharing scheme with the property that the validity of the shares distributed by the dealer can be verified by any party; hence verification is not limited to the respective participants receiving the shares.\" An implementation would follow A Simple Publicly Verifiable Secret Sharing Scheme and its Application to Electronic Voting by Berry Schoenmakers, as well as the DEDIS group at EPFL's Kyber suite written in Go: https://github.com/dedis/kyber/tree/master/share As written, the resulting secret is a random curve point, not a BLS signature or private scalar. In comparison, DFinity's VSS + VRF scheme produces a BLS signature, or even shared private scalar. If the message is known in advance, then PVSS could seemingly produce a BLS signature, although I need to think more about the timing of protocol messages in doing so. If correct, this might answer an important open question of DFinity's Timo Hanke, but maybe not the answer he wants, as PVSS probably need to be run for every signature produced, and DFinity's solution runs the DFG infrequently. Schoenmakers' PVSS avoids pairings but incorporates two rounds of DLEQ proofs. These are complex operations, but might prove faster than DFinity's VSS + VRF scheme, due to pairing based curve. Also, ee need to verify that a type III pairing does not make the protocol insecure and worry if components like the DLEQ proofs need to change for my proposed PVSS + VCF variant. We must also consider if any attacks can influence the public key. Specific random results There are various schemes that produce specific random results without actually producing a shared random value, but the result are necessarily inexact and may need to be simpler or complicate the consensus process. These are mostly applications of verifiable random functions (VRFs), which normally consist of applying a random oracle to the output of a deterministic signature scheme applied to a shared value. Competitive VRFs It's hard to apply VRFs competitively because attackers may delay revealing their results, pretend not to see other's results, use network flood attacks on winners, etc., so roughly the same problems as simple commit and reveal. Ouroboros style \"slot filling\" VRFs We avoid the competitive VRFs issues by using VRFs to fill slots far more numerous than actual winning positions, say slots for block production. There remain related issues with timing and when block producers may skip earlier ones, which require more thought, and may impact other things. Security proofs exist, but tricky UC ones. See Ouroboros paper #2 TODO: link Alistair's VRF leveraging random block selection If we have a network randomness scheme picking block producers or many another specific random results, then they could include a VRF of the block number. As above, each node's only options are to produce a block or not produce a block, so whatever alternatives nodes they have who could produce a block give them influence over the random number, but another user might produce a block first scooping them. Although not a collaborative PRNG per se, this produces random numbers we might argue can be used like collaboratively produced ones. Alistair says we cannot wait for finalisation here, but disagreement results from multiple chains running in parallel. Algorand Algorand has similarities to Tindermint, but selects validators randomly from a pool, instead of using a fixed set, and does a Byzantine agreement for each block. All nodes produce possible blocks and a VRFs helps choose among them. RANDAO Nodes commit to a hash onion and use the layers like a local VRF, revealing one layer whenever doing so allows them to produce a block. RANDAO is less outright computation than local VRFs for verifiers. Also VRFs on different seeds do not aggregate well. RANDAO requires verifiers verify update records for all other verifiers though, which sounds more fragile than a VRF, meaning more sensitive code. RANDAO producers may take a performance hit from storing intermediate hashes, but if so Merkle trees give another similar option. We think orphan blocks prevents us from slashing nodes for skipping their turn, so either all nodes must have a RANDAO contribution for each slot, or else nodes can increases their control over their revealed values by skipping blocks. https://ethresear.ch/t/rng-exploitability-analysis-assuming-pure-randao-based-main-chain/1825/4 https://ethresear.ch/t/randao-beacon-exploitability-analysis-round-2/1980 https://ethresear.ch/t/making-the-randao-less-influenceable-using-a-random-1-bit-clock/2566 Network randomness uses Validator group assignments We think local VRFs, either competitive or slot filling, do not produce an even enough distribution and do not permit us to allocate stake well across validators. So we want a real collaborative PRNGs here. Relevant questions: - Can we produce a security proof for Alistair's VRF leveraging? - How does Ouroboros handle similar situations? - Is PVSS + VRF better? Finality gadget leader assignments Alistair thinks rotation solves everything currently. An alternative is Algorand's local VRF. Block production leader assignments Etherium and Ouroboros handle this with RANDAO and their \"slot filling\" VRF, respectively. We think Ouroboros's VRF has significant implementation advantages, but we may reevaluate after we take performance more into consideration. Fishermen incentivization We envision creating a small about of random missbehavior so that fishermen always have some missbehavior to catch. We should do this with local VRFs so that nobody except the missbehaving node knows in advance, but the levels should be low enough that such missbehavior cannot accumulate. Are there other fishermen related uses? Rob mentioned ordering fishermen, which sounds problematic. Timing TODO: Comment on relative block time vs. more absolute synced clock times. ability to have timeout that depend on network activity? (relative absolute time??) but think about the network conditions We seemingly need a bound on clock skew between validators, which we must enforce using the peer-to-peer network. RANDAO We noticed possible timing issues with RANDAO that impact most other schemes: An attacker can manipulate the timing of their blocks to help maintain a forked chain. TODO: Add details. Ouroboros rate metering Ouroboros paper 3 describes minimum rate requirement for protection against long term attacks. The idea goes that a long-term attacker starts with relatively little stake, so initially they produce blocks slower than the overall network, but as their stake increases this rate goes up. Ouroboros wants to prevent this initial slow start, but this requires Ideas Unavailability game Time or relative time signing","title":"Network randomness schemes"},{"location":"crypto-old/NetRNGs/#network-randomness-schemes","text":"These are our reviews of collaborative random number generators (RNGs) or other networked randomness schemes","title":"Network randomness schemes"},{"location":"crypto-old/NetRNGs/#collaborative-prngs","text":"There are several true collaborative PRNG schemes in which all participants obtain the same output and can then run arbitrary randomised algorithms with identical seeds and results. As a result, we can employ more randomised algorithms that produce exact outputs like permutations of nodes, or require more complexity or tweaking.","title":"Collaborative PRNGs"},{"location":"crypto-old/NetRNGs/#verifiable-delay-functions-vdfs","text":"VDFs employ a slow non-parallelizable computation and provide a PRNG with a proof that some time period elapsed between when seeds were available and when the output was available. Right now, the best ideas for this want a group of unknown order, like maybe the class group of an imaginary quadratic number field. https://crypto.stanford.edu/~dabo/pubs/papers/VDFsurvey.pdf Justin Drake with Etherium research is very interested in VDFs because VDFs provide a collaborative PRNG in which one honest party suffices to produce a secure random number. We currently fear that VDFs deployment strategies have an excessively small security margin. An attacker might for example compute the VDF 10x faster to gains the random number early or even bias it. ASICS might be achieve this. At the extreme, superconducting computing can achieve speeds of 100 GHz with circuits consisting of 10^5 Josephson junctions. You\u2019d need a rather large time window to use a VDFs, and you\u2019d need an even larger time window to handle the highly optimised VDF node equivocating.","title":"Verifiable Delay Functions (VDFs)"},{"location":"crypto-old/NetRNGs/#dfinity-style-vss-vrf","text":"We might not fully understand the VRF's roll here because the public key correctness ultimately depends on the VSS based DKG and this depends upon validator nodes raising accusations. We believe they do this so that one DKG yields numerous VRF outputs, maybe because the DKG is considerably more expensive, or not amenable to some optimisations. Almost all the pure cryptography implementation work consists of following the DKG design in Secure Distributed Key Generation for Discrete-Log Based Cryptosystems by Rosario Gennaro, Stanisl\udbff\udc51aw Jarecki, Hugo Krawczyk, and Tal Rabin. There is no actual production of a BLS signature produced there however, so one should also look over section 7 of DFINITY Technology Overview Series Consensus System (Rev. 1) by Timo Hanke, Mahnush Movahedi and Dominic Williams.","title":"DFinity style VSS + VRF"},{"location":"crypto-old/NetRNGs/#schoenmakers-pvss-ala-epfl","text":"\"A publicly verifiable secret sharing (PVSS) scheme is a verifiable secret sharing scheme with the property that the validity of the shares distributed by the dealer can be verified by any party; hence verification is not limited to the respective participants receiving the shares.\" An implementation would follow A Simple Publicly Verifiable Secret Sharing Scheme and its Application to Electronic Voting by Berry Schoenmakers, as well as the DEDIS group at EPFL's Kyber suite written in Go: https://github.com/dedis/kyber/tree/master/share As written, the resulting secret is a random curve point, not a BLS signature or private scalar. In comparison, DFinity's VSS + VRF scheme produces a BLS signature, or even shared private scalar. If the message is known in advance, then PVSS could seemingly produce a BLS signature, although I need to think more about the timing of protocol messages in doing so. If correct, this might answer an important open question of DFinity's Timo Hanke, but maybe not the answer he wants, as PVSS probably need to be run for every signature produced, and DFinity's solution runs the DFG infrequently. Schoenmakers' PVSS avoids pairings but incorporates two rounds of DLEQ proofs. These are complex operations, but might prove faster than DFinity's VSS + VRF scheme, due to pairing based curve. Also, ee need to verify that a type III pairing does not make the protocol insecure and worry if components like the DLEQ proofs need to change for my proposed PVSS + VCF variant. We must also consider if any attacks can influence the public key.","title":"Schoenmakers PVSS ala EPFL"},{"location":"crypto-old/NetRNGs/#specific-random-results","text":"There are various schemes that produce specific random results without actually producing a shared random value, but the result are necessarily inexact and may need to be simpler or complicate the consensus process. These are mostly applications of verifiable random functions (VRFs), which normally consist of applying a random oracle to the output of a deterministic signature scheme applied to a shared value.","title":"Specific random results"},{"location":"crypto-old/NetRNGs/#competitive-vrfs","text":"It's hard to apply VRFs competitively because attackers may delay revealing their results, pretend not to see other's results, use network flood attacks on winners, etc., so roughly the same problems as simple commit and reveal.","title":"Competitive VRFs"},{"location":"crypto-old/NetRNGs/#ouroboros-style-slot-filling-vrfs","text":"We avoid the competitive VRFs issues by using VRFs to fill slots far more numerous than actual winning positions, say slots for block production. There remain related issues with timing and when block producers may skip earlier ones, which require more thought, and may impact other things. Security proofs exist, but tricky UC ones. See Ouroboros paper #2 TODO: link","title":"Ouroboros style \"slot filling\" VRFs"},{"location":"crypto-old/NetRNGs/#alistairs-vrf-leveraging-random-block-selection","text":"If we have a network randomness scheme picking block producers or many another specific random results, then they could include a VRF of the block number. As above, each node's only options are to produce a block or not produce a block, so whatever alternatives nodes they have who could produce a block give them influence over the random number, but another user might produce a block first scooping them. Although not a collaborative PRNG per se, this produces random numbers we might argue can be used like collaboratively produced ones. Alistair says we cannot wait for finalisation here, but disagreement results from multiple chains running in parallel.","title":"Alistair's VRF leveraging random block selection"},{"location":"crypto-old/NetRNGs/#algorand","text":"Algorand has similarities to Tindermint, but selects validators randomly from a pool, instead of using a fixed set, and does a Byzantine agreement for each block. All nodes produce possible blocks and a VRFs helps choose among them.","title":"Algorand"},{"location":"crypto-old/NetRNGs/#randao","text":"Nodes commit to a hash onion and use the layers like a local VRF, revealing one layer whenever doing so allows them to produce a block. RANDAO is less outright computation than local VRFs for verifiers. Also VRFs on different seeds do not aggregate well. RANDAO requires verifiers verify update records for all other verifiers though, which sounds more fragile than a VRF, meaning more sensitive code. RANDAO producers may take a performance hit from storing intermediate hashes, but if so Merkle trees give another similar option. We think orphan blocks prevents us from slashing nodes for skipping their turn, so either all nodes must have a RANDAO contribution for each slot, or else nodes can increases their control over their revealed values by skipping blocks. https://ethresear.ch/t/rng-exploitability-analysis-assuming-pure-randao-based-main-chain/1825/4 https://ethresear.ch/t/randao-beacon-exploitability-analysis-round-2/1980 https://ethresear.ch/t/making-the-randao-less-influenceable-using-a-random-1-bit-clock/2566","title":"RANDAO"},{"location":"crypto-old/NetRNGs/#network-randomness-uses","text":"","title":"Network randomness uses"},{"location":"crypto-old/NetRNGs/#validator-group-assignments","text":"We think local VRFs, either competitive or slot filling, do not produce an even enough distribution and do not permit us to allocate stake well across validators. So we want a real collaborative PRNGs here. Relevant questions: - Can we produce a security proof for Alistair's VRF leveraging? - How does Ouroboros handle similar situations? - Is PVSS + VRF better?","title":"Validator group assignments"},{"location":"crypto-old/NetRNGs/#finality-gadget-leader-assignments","text":"Alistair thinks rotation solves everything currently. An alternative is Algorand's local VRF.","title":"Finality gadget leader assignments"},{"location":"crypto-old/NetRNGs/#block-production-leader-assignments","text":"Etherium and Ouroboros handle this with RANDAO and their \"slot filling\" VRF, respectively. We think Ouroboros's VRF has significant implementation advantages, but we may reevaluate after we take performance more into consideration.","title":"Block production leader assignments"},{"location":"crypto-old/NetRNGs/#fishermen-incentivization","text":"We envision creating a small about of random missbehavior so that fishermen always have some missbehavior to catch. We should do this with local VRFs so that nobody except the missbehaving node knows in advance, but the levels should be low enough that such missbehavior cannot accumulate. Are there other fishermen related uses? Rob mentioned ordering fishermen, which sounds problematic.","title":"Fishermen incentivization"},{"location":"crypto-old/NetRNGs/#timing","text":"TODO: Comment on relative block time vs. more absolute synced clock times. ability to have timeout that depend on network activity? (relative absolute time??) but think about the network conditions We seemingly need a bound on clock skew between validators, which we must enforce using the peer-to-peer network.","title":"Timing"},{"location":"crypto-old/NetRNGs/#randao_1","text":"We noticed possible timing issues with RANDAO that impact most other schemes: An attacker can manipulate the timing of their blocks to help maintain a forked chain. TODO: Add details.","title":"RANDAO"},{"location":"crypto-old/NetRNGs/#ouroboros-rate-metering","text":"Ouroboros paper 3 describes minimum rate requirement for protection against long term attacks. The idea goes that a long-term attacker starts with relatively little stake, so initially they produce blocks slower than the overall network, but as their stake increases this rate goes up. Ouroboros wants to prevent this initial slow start, but this requires","title":"Ouroboros rate metering"},{"location":"crypto-old/NetRNGs/#ideas","text":"Unavailability game Time or relative time signing","title":"Ideas"},{"location":"keys/0-intro/","text":"Signing keys in Polkadot In this post, we shall first give a high level view of the various signing keys planned for use in Polkadot. We then turn the discussion towards the certificate chain that stretches between staked account keys and the session keys used for our proof-of-stake design. In other words, we aim to lay out the important questions on the \"glue\" between keys rolls here, but first this requires introducing the full spectrum of key rolls. We have roughly four cryptographic layers in Polkadot: Account keys are owned by users and tied to one actual dot denominated account on Polkadot. Accounts could be staked/bonded, unstaked/unbonded, or unstaking/unbonding, but only an unstaked/unbonded account key can transfer dots from one account to another. Nominator keys provide a certificate chain between staked/bonded account keys and the session keys used by nodes in block production or validating. As nominator keys cannot transfer dots, they insulate account keys, which may remain air gapped, from nodes actually running on the internet. Session keys are actually several keys kept together that provide the various signing functions required by validators, including a couple types of verifiable random function (VRF) keys. Transport layer static keys are used by libp2p to authenticate connections between nodes. We shall either certify these with the session key or perhaps include them directly in the session key.","title":"Signing keys in Polkadot"},{"location":"keys/0-intro/#signing-keys-in-polkadot","text":"In this post, we shall first give a high level view of the various signing keys planned for use in Polkadot. We then turn the discussion towards the certificate chain that stretches between staked account keys and the session keys used for our proof-of-stake design. In other words, we aim to lay out the important questions on the \"glue\" between keys rolls here, but first this requires introducing the full spectrum of key rolls. We have roughly four cryptographic layers in Polkadot: Account keys are owned by users and tied to one actual dot denominated account on Polkadot. Accounts could be staked/bonded, unstaked/unbonded, or unstaking/unbonding, but only an unstaked/unbonded account key can transfer dots from one account to another. Nominator keys provide a certificate chain between staked/bonded account keys and the session keys used by nodes in block production or validating. As nominator keys cannot transfer dots, they insulate account keys, which may remain air gapped, from nodes actually running on the internet. Session keys are actually several keys kept together that provide the various signing functions required by validators, including a couple types of verifiable random function (VRF) keys. Transport layer static keys are used by libp2p to authenticate connections between nodes. We shall either certify these with the session key or perhaps include them directly in the session key.","title":"Signing keys in Polkadot"},{"location":"keys/1-accounts-more/","text":"Account signatures and keys in Polkadot We believe Polkadot accounts should primarily use Schnorr signatures with both public keys and the R point in the signature encoded using the Ristretto point compression for the Ed25519 curve. We should collaborate with the dalek ecosystem for which Ristretto was developed, but provide a simpler signature crate, for which schnorr-dalek provides a first step. Schnorr signatures We prefer Schnorr signatures because they satisfy the Bitcoin Schnoor wishlist and work fine with extremely secure curves, like secp256k1 or the Ed25519 curve. You could do fancier tricks, including like aggregation, with a pairing based curve like BLS12-381 and the BLS signature scheme. These curves are slower for single verifications, and worse accounts should last decades while pairing friendly curves should be expected become less secure as number theory advances. There is one sacrifice we make by choosing Schnorr signatures over ECDSA signatures for account keys: Both require 64 bytes, but only ECDSA signatures communicate their public key . There are obsolete Schnorr variants that support recovering the public key from a signature , but they break important functionality like hierarchical deterministic key derivation . In consequence, Schnorr signatures often take an extra 32 bytes for the public key. In exchange, we gain a slightly faster signature scheme with far simpler batch verification than ECDSA batch verification and more natural threshold and multi-signatures, as well as tricks used by payment channels. I also foresee the presence of this public key data may improve locality in block verification, possibly openning up larger optimisations. Yet most importantly, we can protect Schnorr signatures using both the derandomization tricks of EdDSA along with a random number generator, which gives us stronger side-channel protections than conventional ECDSA schemes provide. If we ever do want to support ECDSA as well, then we would first explore improvements in side-channel protections like rfc6979 , along with concerns like batch verification, etc. Curves There are two normal curve choices for accounts on a blockchain system, either secp256k1 or the Ed25519 curve, so we confine our discussion to them. If you wanted slightly more speed, you might choose FourQ, but it sounds excessive for blockchains, implementations are rare, and it appears covered by older but not quite expired patents. Also, you might choose Zcash's JubJub if you wanted fast signature verification in zkSNARKs, but that's not on our roadmap for Polkadot, and Jubjub also lacks many implementations. How much secp256k1 support? We need some minimal support for secp256k1 keys because token sale accounts are tied to secp256k1 keys on Ethereum, so some \"account\" type must necessarily use secp256k1 keys. At the same time, we should not encourage using the same private keys on Ethereum and Polkadot. We might pressure users into switching key types in numerous ways, like secp256k1 accounts need not support balance increases, or might not support anything but replacing themselves with an ed25519 key. There are conceivable reasons for fuller secp256k1 support though, like wanting ethereum smart contracts to verify some signatures on Polkadot. We might support secp256k1 accounts with limited functionality, but consider expanding that functionality if such use cases arise. Is secp256k1 risky? There are two theoretical reasons for preferring an twisted Edwards curve over secp256k1: First, secp256k1 has a small CM field discriminant , which might yield better attacks in the distant future. Second, secp256k1 has fairly rigid paramater choices but not the absolute best . I do not believe either to be serious cause for concern. Among more practical curve weaknesses, secp256k1 does have twist security which eliminates many attack classes. I foresee only one substancial reason for avoiding secp256k1: All short Weierstrass curves like secp256k1 have incomplete addition formulas , meaning certain curve points cannot be added to other curve points. As a result, addition code must check for failures, but these checks make writing constant time code harder. We could examine any secp256k1 library we use in Polkadot to ensure it both does these checks and has constant-time code. We cannot however ensure that all implementations used by third party wallet software does so. I believe incomplete addition formulas looks relatively harmless when used for simple Schnorr signatures, although forgery attacks might exist. I'd worry more however if we began using secp256k1 for less well explored protocols, like multi-signaturtes and key derivation. We ware about such use cases however, especially those listed in the Bitcoin Schnoor wishlist . Is Ed25519 risky? Aka use Ristretto Any elliptic curve used in cryptography has order h*l where l is a big prime, normally close to a power of two, and h is some very small number called the cofactor. Almost all protocol implementations are complicated by these cofactors, so implementing complex protocols is safer on curves with cofactor h=1 like secp256k1. The Ed25519 curve has cofactor 8 but a simple convention called \"clamping\" that makes two particularly common protocols secure. We must restrict or drop \"clamping\" for more complex protocols, like multi-signaturtes and key derivation, or anything else in the Bitcoin Schnoor wishlist . If we simple dropped \"clamping\" then we'd make implementing protocols harder, but luckily the Ristretto encoding for the Ed25519 curve ensures we avoid any curve points with 2-torsion. I thus recommend: - our secret key continue being Ed25519 \"expanded\" secret keys, while - our on-chain encoding, aka \"point compression\" becomes Ristretto for both public keys and the R component of Schnoor signatures. In principle, we could use the usual Ed25519 \"mini\" secret keys for simple use cases, but not when doing key derivation. We could thus easily verify standrad Ed25519 signatures with Ristretto encoded public keys. We should ideally use Ristretto throughout instead of the standard Ed25519 point compression. In fact, we can import standard Ed25519 compressed points like I do here but this requires the scalar exponentiation done in the is_torsion_free method , which runs slower than normal signature verification. We might ideally do this only for key migration between PoCs. Ristretto is far simpler than the Ed25519 curve itself, so Ristretto can be added to Ed25519 implementations, but the curve25519-dalek crate already provides a highly optimised rust implementation. Zero-knowledge proofs in the dalek ecosystem In fact, the dalek ecosystem has an remarkably well designed infrastructure for zero-knowledge proofs without pairings. See: https://medium.com/interstellar/bulletproofs-pre-release-fcb1feb36d4b https://medium.com/interstellar/programmable-constraint-systems-for-bulletproofs-365b9feb92f7 All these crates use Ristretto points so using Ristretto for account public keys ourselves gives us the most advanced tools for building protocols not based on pairings, meaning that use our account keys. In principle, these tools might be abstracted for twisted Edwards curves like FourQ and Zcash's Jubjub, but yu might loose some batching operations in abstracting them for short Weierstrass curves like secp256k1.","title":"Account signatures and keys in Polkadot"},{"location":"keys/1-accounts-more/#account-signatures-and-keys-in-polkadot","text":"We believe Polkadot accounts should primarily use Schnorr signatures with both public keys and the R point in the signature encoded using the Ristretto point compression for the Ed25519 curve. We should collaborate with the dalek ecosystem for which Ristretto was developed, but provide a simpler signature crate, for which schnorr-dalek provides a first step.","title":"Account signatures and keys in Polkadot"},{"location":"keys/1-accounts-more/#schnorr-signatures","text":"We prefer Schnorr signatures because they satisfy the Bitcoin Schnoor wishlist and work fine with extremely secure curves, like secp256k1 or the Ed25519 curve. You could do fancier tricks, including like aggregation, with a pairing based curve like BLS12-381 and the BLS signature scheme. These curves are slower for single verifications, and worse accounts should last decades while pairing friendly curves should be expected become less secure as number theory advances. There is one sacrifice we make by choosing Schnorr signatures over ECDSA signatures for account keys: Both require 64 bytes, but only ECDSA signatures communicate their public key . There are obsolete Schnorr variants that support recovering the public key from a signature , but they break important functionality like hierarchical deterministic key derivation . In consequence, Schnorr signatures often take an extra 32 bytes for the public key. In exchange, we gain a slightly faster signature scheme with far simpler batch verification than ECDSA batch verification and more natural threshold and multi-signatures, as well as tricks used by payment channels. I also foresee the presence of this public key data may improve locality in block verification, possibly openning up larger optimisations. Yet most importantly, we can protect Schnorr signatures using both the derandomization tricks of EdDSA along with a random number generator, which gives us stronger side-channel protections than conventional ECDSA schemes provide. If we ever do want to support ECDSA as well, then we would first explore improvements in side-channel protections like rfc6979 , along with concerns like batch verification, etc.","title":"Schnorr signatures"},{"location":"keys/1-accounts-more/#curves","text":"There are two normal curve choices for accounts on a blockchain system, either secp256k1 or the Ed25519 curve, so we confine our discussion to them. If you wanted slightly more speed, you might choose FourQ, but it sounds excessive for blockchains, implementations are rare, and it appears covered by older but not quite expired patents. Also, you might choose Zcash's JubJub if you wanted fast signature verification in zkSNARKs, but that's not on our roadmap for Polkadot, and Jubjub also lacks many implementations.","title":"Curves"},{"location":"keys/1-accounts-more/#how-much-secp256k1-support","text":"We need some minimal support for secp256k1 keys because token sale accounts are tied to secp256k1 keys on Ethereum, so some \"account\" type must necessarily use secp256k1 keys. At the same time, we should not encourage using the same private keys on Ethereum and Polkadot. We might pressure users into switching key types in numerous ways, like secp256k1 accounts need not support balance increases, or might not support anything but replacing themselves with an ed25519 key. There are conceivable reasons for fuller secp256k1 support though, like wanting ethereum smart contracts to verify some signatures on Polkadot. We might support secp256k1 accounts with limited functionality, but consider expanding that functionality if such use cases arise.","title":"How much secp256k1 support?"},{"location":"keys/1-accounts-more/#is-secp256k1-risky","text":"There are two theoretical reasons for preferring an twisted Edwards curve over secp256k1: First, secp256k1 has a small CM field discriminant , which might yield better attacks in the distant future. Second, secp256k1 has fairly rigid paramater choices but not the absolute best . I do not believe either to be serious cause for concern. Among more practical curve weaknesses, secp256k1 does have twist security which eliminates many attack classes. I foresee only one substancial reason for avoiding secp256k1: All short Weierstrass curves like secp256k1 have incomplete addition formulas , meaning certain curve points cannot be added to other curve points. As a result, addition code must check for failures, but these checks make writing constant time code harder. We could examine any secp256k1 library we use in Polkadot to ensure it both does these checks and has constant-time code. We cannot however ensure that all implementations used by third party wallet software does so. I believe incomplete addition formulas looks relatively harmless when used for simple Schnorr signatures, although forgery attacks might exist. I'd worry more however if we began using secp256k1 for less well explored protocols, like multi-signaturtes and key derivation. We ware about such use cases however, especially those listed in the Bitcoin Schnoor wishlist .","title":"Is secp256k1 risky?"},{"location":"keys/1-accounts-more/#is-ed25519-risky-aka-use-ristretto","text":"Any elliptic curve used in cryptography has order h*l where l is a big prime, normally close to a power of two, and h is some very small number called the cofactor. Almost all protocol implementations are complicated by these cofactors, so implementing complex protocols is safer on curves with cofactor h=1 like secp256k1. The Ed25519 curve has cofactor 8 but a simple convention called \"clamping\" that makes two particularly common protocols secure. We must restrict or drop \"clamping\" for more complex protocols, like multi-signaturtes and key derivation, or anything else in the Bitcoin Schnoor wishlist . If we simple dropped \"clamping\" then we'd make implementing protocols harder, but luckily the Ristretto encoding for the Ed25519 curve ensures we avoid any curve points with 2-torsion. I thus recommend: - our secret key continue being Ed25519 \"expanded\" secret keys, while - our on-chain encoding, aka \"point compression\" becomes Ristretto for both public keys and the R component of Schnoor signatures. In principle, we could use the usual Ed25519 \"mini\" secret keys for simple use cases, but not when doing key derivation. We could thus easily verify standrad Ed25519 signatures with Ristretto encoded public keys. We should ideally use Ristretto throughout instead of the standard Ed25519 point compression. In fact, we can import standard Ed25519 compressed points like I do here but this requires the scalar exponentiation done in the is_torsion_free method , which runs slower than normal signature verification. We might ideally do this only for key migration between PoCs. Ristretto is far simpler than the Ed25519 curve itself, so Ristretto can be added to Ed25519 implementations, but the curve25519-dalek crate already provides a highly optimised rust implementation.","title":"Is Ed25519 risky?  Aka use Ristretto"},{"location":"keys/1-accounts-more/#zero-knowledge-proofs-in-the-dalek-ecosystem","text":"In fact, the dalek ecosystem has an remarkably well designed infrastructure for zero-knowledge proofs without pairings. See: https://medium.com/interstellar/bulletproofs-pre-release-fcb1feb36d4b https://medium.com/interstellar/programmable-constraint-systems-for-bulletproofs-365b9feb92f7 All these crates use Ristretto points so using Ristretto for account public keys ourselves gives us the most advanced tools for building protocols not based on pairings, meaning that use our account keys. In principle, these tools might be abstracted for twisted Edwards curves like FourQ and Zcash's Jubjub, but yu might loose some batching operations in abstracting them for short Weierstrass curves like secp256k1.","title":"Zero-knowledge proofs in the dalek ecosystem"},{"location":"keys/1-accounts/","text":"Account signatures and keys We believe Polkadot accounts should primarily use Schnorr signatures with both public keys and the R point in the signature encoded using the Ristretto point compression for the Ed25519 curve. We should collaborate with the dalek ecosystem for which Ristretto was developed, but provide a simpler signature crate, for which schnorr-dalek provides a first step. I'll write a another comment giving more details behind this choice, but the high level summary goes: Account keys must support the diverse functionality desired of account keys on other systems like Ethereum and Bitcoin. As such, our account keys shall use Schnorr signatures because these support fast batch verification and hierarchical deterministic key derivation ala BIP32 . All features from the Bitcoin Schnoor wishlist provides a case for Schnorr signatures matter too, like interactive threshold and multi-signaturtes, as well as adaptor, and perhaps even blind, signatures for swaps and payment channels. We make conservative curve choices here because account keys must live for decades. In particular, we avoid pairing-based cryptography and BLS signatures for accounts, at the cost of true aggregation of the signatures in a block when verifying blocks, and less interactive threshold and multi-signaturtes. [1]. In the past, there was a tricky choice between the more secure curves: miss-implementation resistance is stronger with Edwards curves, including the Ed25519 curve, but miss-use resistance in stronger when curves have cofactor 1, like secp256k1. In fact, miss-use resistance was historically a major selling point for Ed25519, which itself is a Schnorr variant, but this miss-use resistance extends only so far as the rudimentary signature scheme properties it provided. Yet, any advanced signature scheme functions, beyond batch verification, break precisely due to Ed25519's miss-use resistance. In fact, there are tricks for doing at least hierarchical deterministic key derivation on Ed25519, as implemented in hd-ed25519 , but almost all previous efforts produced insecure results . We observe that secp256k1 provides a good curve choice from among the curves of cofactor 1, which simplify make implementing fancier protocols. We do worry that such curves appear at least slightly weaker than Edwards curves. We worry much more than such curves tend to be harder to implement well, due to having incomplete addition formulas, and thus require more review (see safecurves.cr.yp.to ). We could select only solid implementations for Polkadot itself, but we cannot control the implementations selected elsewhere in our ecosystem, especially by wallet software. In short, we want an Edwards curve but without the cofactor, which do not exist, except.. In Edwards curve of with cofactor 4, Mike Hamburg's Decaf point compression only permits serialising and deserialising points on the subgroup of order $l$, which provides a perfect solution. Ristretto pushes this point compression to cofactor 8, making it applicable to the Ed25519 curve. Implementations exist in both Rust and C . If required in another language, the compression and decompression functions are reasonable to implement using an existing field implementation, and fairly easy to audit. In the author's words, \"Rather than bit-twiddling, point mangling, or otherwise kludged-in ad-hoc fixes, Ristretto is a thin layer that provides protocol implementors with the correct abstraction: a prime-order group.\" [1] Aggregation can dramatically reduce signed message size when applying numerous signatures, but if performance is the only goal then batch verification techniques similar results, and exist for mny signature schemes, including Schnorr. There are clear advantages to reducing interactiveness in threshold and multi-signaturtes, but parachains can always provide these on Polkadot. Importantly, there are numerous weaknesses in all known curves that support pairings, but the single most damning weakness is the pairing $e : G_1 \\times G_2 \\to G_T$ itself. In essence, we use elliptic curves in the first palce because they insulate us somewhat from mathematicians ever advancing understanding of number theory. Yet, any known pairing maps into a group $G_T$ that re-exposes us, so attacks based on index-calculus, etc. improve more quickly. As a real world example, there were weaknesses found in BN curve of the sort used by ZCash during development, so after launch they needed to develop and migrate to a new curve . We expect this to happen again for roughly the same reasons that RSA key sizes increase slowly over time.","title":"1 accounts"},{"location":"keys/1-accounts/#account-signatures-and-keys","text":"We believe Polkadot accounts should primarily use Schnorr signatures with both public keys and the R point in the signature encoded using the Ristretto point compression for the Ed25519 curve. We should collaborate with the dalek ecosystem for which Ristretto was developed, but provide a simpler signature crate, for which schnorr-dalek provides a first step. I'll write a another comment giving more details behind this choice, but the high level summary goes: Account keys must support the diverse functionality desired of account keys on other systems like Ethereum and Bitcoin. As such, our account keys shall use Schnorr signatures because these support fast batch verification and hierarchical deterministic key derivation ala BIP32 . All features from the Bitcoin Schnoor wishlist provides a case for Schnorr signatures matter too, like interactive threshold and multi-signaturtes, as well as adaptor, and perhaps even blind, signatures for swaps and payment channels. We make conservative curve choices here because account keys must live for decades. In particular, we avoid pairing-based cryptography and BLS signatures for accounts, at the cost of true aggregation of the signatures in a block when verifying blocks, and less interactive threshold and multi-signaturtes. [1]. In the past, there was a tricky choice between the more secure curves: miss-implementation resistance is stronger with Edwards curves, including the Ed25519 curve, but miss-use resistance in stronger when curves have cofactor 1, like secp256k1. In fact, miss-use resistance was historically a major selling point for Ed25519, which itself is a Schnorr variant, but this miss-use resistance extends only so far as the rudimentary signature scheme properties it provided. Yet, any advanced signature scheme functions, beyond batch verification, break precisely due to Ed25519's miss-use resistance. In fact, there are tricks for doing at least hierarchical deterministic key derivation on Ed25519, as implemented in hd-ed25519 , but almost all previous efforts produced insecure results . We observe that secp256k1 provides a good curve choice from among the curves of cofactor 1, which simplify make implementing fancier protocols. We do worry that such curves appear at least slightly weaker than Edwards curves. We worry much more than such curves tend to be harder to implement well, due to having incomplete addition formulas, and thus require more review (see safecurves.cr.yp.to ). We could select only solid implementations for Polkadot itself, but we cannot control the implementations selected elsewhere in our ecosystem, especially by wallet software. In short, we want an Edwards curve but without the cofactor, which do not exist, except.. In Edwards curve of with cofactor 4, Mike Hamburg's Decaf point compression only permits serialising and deserialising points on the subgroup of order $l$, which provides a perfect solution. Ristretto pushes this point compression to cofactor 8, making it applicable to the Ed25519 curve. Implementations exist in both Rust and C . If required in another language, the compression and decompression functions are reasonable to implement using an existing field implementation, and fairly easy to audit. In the author's words, \"Rather than bit-twiddling, point mangling, or otherwise kludged-in ad-hoc fixes, Ristretto is a thin layer that provides protocol implementors with the correct abstraction: a prime-order group.\" [1] Aggregation can dramatically reduce signed message size when applying numerous signatures, but if performance is the only goal then batch verification techniques similar results, and exist for mny signature schemes, including Schnorr. There are clear advantages to reducing interactiveness in threshold and multi-signaturtes, but parachains can always provide these on Polkadot. Importantly, there are numerous weaknesses in all known curves that support pairings, but the single most damning weakness is the pairing $e : G_1 \\times G_2 \\to G_T$ itself. In essence, we use elliptic curves in the first palce because they insulate us somewhat from mathematicians ever advancing understanding of number theory. Yet, any known pairing maps into a group $G_T$ that re-exposes us, so attacks based on index-calculus, etc. improve more quickly. As a real world example, there were weaknesses found in BN curve of the sort used by ZCash during development, so after launch they needed to develop and migrate to a new curve . We expect this to happen again for roughly the same reasons that RSA key sizes increase slowly over time.","title":"Account signatures and keys"},{"location":"keys/2-staking/","text":"Nominator keys In some sense, all public keys derive their authority from some combination of ceremonies and certificates, with certificate root keys deriving tehir authority entirely from ceremonies. As an example, trust-on-first-use schemes might be considered a pair of cerimonies, the key being associated to an identity first, and the threat of other comparing keys fingerprints. We apply this perspective to a consensus algorithm for a proof-of-stake blockchains like polkadot by regarding the chain itself as one large ceremony and treating the staked account as the root of trust. We then have certificates issued by these staked account keys that authenticate both the session keys used by Polkadot validators and block producers, as well as the long-term transport layer authentication keys required by TLS or Noise (see concerns about libp2p's secio). We must support, or may even require that, these session keys and TLS keys rotate periodically. At thesame time, we must support staked account keys being air gapped, which prevents them from signing anything regularly. In consequence, we require a layer layer called nominator keys that lies strictly between staked account keys and session keys. In this post, we shall discuss the certificate scheme for delegating from staked account keys to nominator keys and delegating from nominator keys to session keys, which includes several unanswered questions. In principle, any certificate format should work for nominator keys, but some certificate schemes provide more flexibility, while others save space. We do not require much flexibility per se, but at least some of these certificates should live inside the staked account, and have some associated data: block hash and height when staked, unstaking block hash and height, if unstaking, and permissions, like only block production, validator nomination, and validator operator. One vs two layer We can support nominated proof-of-stake with only one layer per se. We would have validator operator nominator keys point directly to their validator's current session keys, while nominator keys that only nominate would likely point to validator operator's nominator keys. We expect all nominator keys act as owners for a block production node's session key because we do not permit delegation for block production. We could require another layer, either by envisioning the session key itself as two layer, or by adding a second layer of nominator key. I think a two layer session key simplifies session key rollover, which improves forward security and thus reduces the benefits of compromising nodes. Certificate location We could either store certificates with account data, or else provide certificates in protocol interactions, but almost surely the certificate delegating from the staked account to the nominator key belongs in the account data. We should take care with the certificates from the nominator key to the session key because the session key requires a proof-of-possesion. If we place them into the account, then there is a temptation to trust them and not check the proof-of-possesion ourselves. We cannot necessarily trust the chain for proofs-of-possesion because doing so might provides escalation for attackers who succeed in posting any invalid data. If we provide them in interactions then there is a temptation to check the proof-of-possesion repeatedly. We should evaluate either attaching a self-checked flag to the staked account database vs storing session keys in some self-checked account database separate from the account database for which nodes trust the chain. Certificate size We could save some space by using implicit certificates to issue nominator keys, but we consider our initial implementation in schnorr-dalek/src/cert.rs insufficient. In essence, an accounts nominator key could be defined by an additional 32 bytes attached to the account, along with any associated data. We need to hold a conversation about (a) what form this associated data should take, and (b) if the space savings are worth the complexity of an implicit certificates scheme, mostly reviewing the literature . We clearly need non-implicit certificates for non-owning nominators. As a result, we might actually reduce the code complexity by not using implicit certificates in the nomination process. We might then achieve better code reuse by not using implicit certificates anywhere.","title":"2 staking"},{"location":"keys/2-staking/#nominator-keys","text":"In some sense, all public keys derive their authority from some combination of ceremonies and certificates, with certificate root keys deriving tehir authority entirely from ceremonies. As an example, trust-on-first-use schemes might be considered a pair of cerimonies, the key being associated to an identity first, and the threat of other comparing keys fingerprints. We apply this perspective to a consensus algorithm for a proof-of-stake blockchains like polkadot by regarding the chain itself as one large ceremony and treating the staked account as the root of trust. We then have certificates issued by these staked account keys that authenticate both the session keys used by Polkadot validators and block producers, as well as the long-term transport layer authentication keys required by TLS or Noise (see concerns about libp2p's secio). We must support, or may even require that, these session keys and TLS keys rotate periodically. At thesame time, we must support staked account keys being air gapped, which prevents them from signing anything regularly. In consequence, we require a layer layer called nominator keys that lies strictly between staked account keys and session keys. In this post, we shall discuss the certificate scheme for delegating from staked account keys to nominator keys and delegating from nominator keys to session keys, which includes several unanswered questions. In principle, any certificate format should work for nominator keys, but some certificate schemes provide more flexibility, while others save space. We do not require much flexibility per se, but at least some of these certificates should live inside the staked account, and have some associated data: block hash and height when staked, unstaking block hash and height, if unstaking, and permissions, like only block production, validator nomination, and validator operator.","title":"Nominator keys"},{"location":"keys/2-staking/#one-vs-two-layer","text":"We can support nominated proof-of-stake with only one layer per se. We would have validator operator nominator keys point directly to their validator's current session keys, while nominator keys that only nominate would likely point to validator operator's nominator keys. We expect all nominator keys act as owners for a block production node's session key because we do not permit delegation for block production. We could require another layer, either by envisioning the session key itself as two layer, or by adding a second layer of nominator key. I think a two layer session key simplifies session key rollover, which improves forward security and thus reduces the benefits of compromising nodes.","title":"One vs two layer"},{"location":"keys/2-staking/#certificate-location","text":"We could either store certificates with account data, or else provide certificates in protocol interactions, but almost surely the certificate delegating from the staked account to the nominator key belongs in the account data. We should take care with the certificates from the nominator key to the session key because the session key requires a proof-of-possesion. If we place them into the account, then there is a temptation to trust them and not check the proof-of-possesion ourselves. We cannot necessarily trust the chain for proofs-of-possesion because doing so might provides escalation for attackers who succeed in posting any invalid data. If we provide them in interactions then there is a temptation to check the proof-of-possesion repeatedly. We should evaluate either attaching a self-checked flag to the staked account database vs storing session keys in some self-checked account database separate from the account database for which nodes trust the chain.","title":"Certificate location"},{"location":"keys/2-staking/#certificate-size","text":"We could save some space by using implicit certificates to issue nominator keys, but we consider our initial implementation in schnorr-dalek/src/cert.rs insufficient. In essence, an accounts nominator key could be defined by an additional 32 bytes attached to the account, along with any associated data. We need to hold a conversation about (a) what form this associated data should take, and (b) if the space savings are worth the complexity of an implicit certificates scheme, mostly reviewing the literature . We clearly need non-implicit certificates for non-owning nominators. As a result, we might actually reduce the code complexity by not using implicit certificates in the nomination process. We might then achieve better code reuse by not using implicit certificates anywhere.","title":"Certificate size"},{"location":"keys/3-session/","text":"Session keys A session public key should consist of roughly four public keys types: Ristretto Schnorr public key (32 bytes public keys, 64 byte signatures, 96 byte VRFs) We issue these from the nominator keys acting as validator operators. We might use an implicit certificate but doing so either restricts us to one validator operator, or else increases code complexity and forces a primary validator operator. Implicit certificates also make session key records impossible to authenticate without the nominator account, but this sounds desirable. We know signers can easily batch numerous VRF outputs into a single proof with these, ala CloudFlare's Privacy Pass. If we employ these VRFs for block production then signers could periodically publish a \"sync digest\" that consolidated thousands of their past block production VRFs into a single check, which improves syncing speed. There is also an avenue to batch verify these VRFs by multiply signers, but it requires enlarging the VRF output and proofs from from 96 to 128 bytes. Small curve of BLS12-381 (48 byte public keys, 96 byte signatures) Aggregated signatures verify can faster when using this key if the signer set for a particular message is large but irregularly composed, as in GRANDPA. Actual signatures are slower than the opposite orientation, and non-constant time extension field arithmetic makes them even slower, or more risky. Aggregating signatures on the same message like this incurs malleability risks too. We also envision using this scheme in some fishermen schemes. We should consider slothful reduction as discussed in https://github.com/zkcrypto/pairing/issues/98 for these eventually, but initially key splitting should provide solid protection against timing attacks, but with even slower signature speed. Big curve of BLS12-381 (96 bytes public keys, 48 byte signatures) Aggregated signatures in which we verify many messages by the same signer verify considerably faster when using this key. We might use these for block production VRFs because they aggregating over the same signer sounds useful for syncing. Initially, we envisioned aggregation being useful for some VRF non-winner proofs designs, but our new non-winner proof design mostly avoids this requirement. Right now, we favor the Ristretto Schnorr VRF for block production because individual instances verify faster and it provides rather extreme batching over the same signer already. We also expect faster aggregate verification from these when signer sets get repeated frequently, so conceivably these make sense for some settings in which small curve keys initially sound optimal. We envision signature aggregation being \"wild\" in GRANDPA, so the small curve key still sounds best there. Authentication key for the transport layer. We might ideally include node identity form libp2p, but secio handles authentication poorly ( see the secio discussion ). A session public key record has a prefix consisting of the above three keys, along with a certificate from the validator operator on the Ristretto Schnorr public key and some previous block hash and height. We follow this prefix with a first signature block consisting two BLS signatures on the prefix, one by each the BLS keys. We close the session public key record with a second signature block consisting of a Ristretto Schnorr signature on both the prefix and first signature block. In this way, we may rotate our BLS12-381 keys without rotating our Ristretto Schnorr public key, possibly buying us some forward security. We include the recent block hash in the certificate, so that if the chain were trusted for proofs-of-possession then attackers cannot place rogue keys that attack honestly created session keys created after their fork. We recommend against trusting the chain for proofs-of-possession however because including some recent block hash like this only helps against longer range attacks. We still lack any wonderful aggregation strategy for block production VRFs, so they may default to Ristretto Schnorr VRFs. In this case, the Ristretto Schnorr session key component living longer also help minimize attacks on our random beacon.","title":"3 session"},{"location":"keys/3-session/#session-keys","text":"A session public key should consist of roughly four public keys types: Ristretto Schnorr public key (32 bytes public keys, 64 byte signatures, 96 byte VRFs) We issue these from the nominator keys acting as validator operators. We might use an implicit certificate but doing so either restricts us to one validator operator, or else increases code complexity and forces a primary validator operator. Implicit certificates also make session key records impossible to authenticate without the nominator account, but this sounds desirable. We know signers can easily batch numerous VRF outputs into a single proof with these, ala CloudFlare's Privacy Pass. If we employ these VRFs for block production then signers could periodically publish a \"sync digest\" that consolidated thousands of their past block production VRFs into a single check, which improves syncing speed. There is also an avenue to batch verify these VRFs by multiply signers, but it requires enlarging the VRF output and proofs from from 96 to 128 bytes. Small curve of BLS12-381 (48 byte public keys, 96 byte signatures) Aggregated signatures verify can faster when using this key if the signer set for a particular message is large but irregularly composed, as in GRANDPA. Actual signatures are slower than the opposite orientation, and non-constant time extension field arithmetic makes them even slower, or more risky. Aggregating signatures on the same message like this incurs malleability risks too. We also envision using this scheme in some fishermen schemes. We should consider slothful reduction as discussed in https://github.com/zkcrypto/pairing/issues/98 for these eventually, but initially key splitting should provide solid protection against timing attacks, but with even slower signature speed. Big curve of BLS12-381 (96 bytes public keys, 48 byte signatures) Aggregated signatures in which we verify many messages by the same signer verify considerably faster when using this key. We might use these for block production VRFs because they aggregating over the same signer sounds useful for syncing. Initially, we envisioned aggregation being useful for some VRF non-winner proofs designs, but our new non-winner proof design mostly avoids this requirement. Right now, we favor the Ristretto Schnorr VRF for block production because individual instances verify faster and it provides rather extreme batching over the same signer already. We also expect faster aggregate verification from these when signer sets get repeated frequently, so conceivably these make sense for some settings in which small curve keys initially sound optimal. We envision signature aggregation being \"wild\" in GRANDPA, so the small curve key still sounds best there. Authentication key for the transport layer. We might ideally include node identity form libp2p, but secio handles authentication poorly ( see the secio discussion ). A session public key record has a prefix consisting of the above three keys, along with a certificate from the validator operator on the Ristretto Schnorr public key and some previous block hash and height. We follow this prefix with a first signature block consisting two BLS signatures on the prefix, one by each the BLS keys. We close the session public key record with a second signature block consisting of a Ristretto Schnorr signature on both the prefix and first signature block. In this way, we may rotate our BLS12-381 keys without rotating our Ristretto Schnorr public key, possibly buying us some forward security. We include the recent block hash in the certificate, so that if the chain were trusted for proofs-of-possession then attackers cannot place rogue keys that attack honestly created session keys created after their fork. We recommend against trusting the chain for proofs-of-possession however because including some recent block hash like this only helps against longer range attacks. We still lack any wonderful aggregation strategy for block production VRFs, so they may default to Ristretto Schnorr VRFs. In this case, the Ristretto Schnorr session key component living longer also help minimize attacks on our random beacon.","title":"Session keys"},{"location":"keys/4-secio/","text":"Transport layer authentication - libp2p's SECIO We must authenticate connections as the transport layer from session keys, which could happen in several ways, like signing a hash provided by the transport layer. A priori, we could simplify everything if the session key simply included the static key used in authentication component of the transport layer's key exchange, which might help avoid some security mistakes elsewhere too. There are numerous transports for libp2p, but only QUIC was actually designed to be secure. Instead, one routes traffic through libp2p's secio protocol . We trust QUIC's cryptographic layer which is TLS 1.3, but secio itself is a home brew job with no serious security analysis, which usually goes poorly . There has been minimal discussion of secio's security but Dominic Tarr raised some concerns in the original pull request . I'll reraise several concerns from that discussion: First, there is no effort made to hide secio node keys because \"IPFS has no interest in [metadata privacy]\" according to Brendan McMillion, so nodes leak their identities onto the raw internet. We think identifying nodes sounds easy anyways, but at minimum this invites attacks. There is an asymmetry to key exchanges that leaks less by first establishing a secure channel and only then authenticating. We might reasonably break this asymmetry by deciding that specific roles require more privacy. We might for example help protect validator operators or improve censorship resistance in some cases, like fishermen. Second, there is cipher suit agility in secio, at minimum in their use of multihash, but maybe even in the underlying key exchange itself. We've seen numerous attacks on TLS <= 1.2 due to cipher suit agility, especially the downgrade attacks. I therefore strongly recommend using TLS 1.3 if cipher suit agility is required. We could likely version the entire protocol though, thus avoiding any cipher suit agility. In any case, constructs like multihash should be considered hazardous in even basic key exchanges, but certainly in more advanced protocols involving signatures or associated data. Third, there are no ACKs in secio which might yield attacks when a higher level protocol actually depends upon the underlying insecure transport's own ACKs. I suppose UDP transport support already requires higher level protocol handle any required ACKs themselves anyways. ( related ). As QUIC uses UDP only, we could add TCP based transport that uses TLS 1.3, perhaps by extending libp2p's existing transport with support for TLS 1.3, or perhaps adding a more flexible TLS 1.3 layer. We might prefer a flexible TLS 1.3 layer over conventional TLS integration into libp2p extending transports because our authentication privacy demands might work differently from TLS's server oriented model. We could identify some reasonable Noise variant , if avoiding the complexity of TLS sounds like a priority and ACKs are always handled by higher layers. I believe Noise XX fits the blockchain context well, due to Alice and Bob roles being easily reversible, improved modularity, and more asynchronous key certification from on-chain data. At the extreme, we could imagine identifying particular handshakes for particular interactions though, like GRANDPA using KK and fishermen using NK. In short, we should make a new years resolution to replace secio, with our two simplest routes being either TLS 1.3 or Noise XX. Aside from these authentication repairs, there are two additional directions for possible future work: Post-quantum key exchange. We'd likely employ LWE scheme here. Right now, CSIDH remains young and slow, but the small key size and long-term keys claims indicate that CSIDH might integrate better with Noise and blockchains. I'd skip the existing specification for integrating Noise with New Hope Simple. Adam Langely has good arguments for selecting the NTRU variant NRSS+SXY for Google's CECPQ2 experiment . I the module-LWE Kyber Forward-security. There is some multi-hop message forwarding in libp2p, but it provides only another addressing technique, not a true connection abstraction layer like say GNUNet's CADET layer. CADET actually employs the Axolotl forward secure ratchet. I'm always a fan of both post-quantum and forward security for encryption, but the benefits might prove minimal in our context.","title":"4 secio"},{"location":"keys/4-secio/#transport-layer-authentication-libp2ps-secio","text":"We must authenticate connections as the transport layer from session keys, which could happen in several ways, like signing a hash provided by the transport layer. A priori, we could simplify everything if the session key simply included the static key used in authentication component of the transport layer's key exchange, which might help avoid some security mistakes elsewhere too. There are numerous transports for libp2p, but only QUIC was actually designed to be secure. Instead, one routes traffic through libp2p's secio protocol . We trust QUIC's cryptographic layer which is TLS 1.3, but secio itself is a home brew job with no serious security analysis, which usually goes poorly . There has been minimal discussion of secio's security but Dominic Tarr raised some concerns in the original pull request . I'll reraise several concerns from that discussion: First, there is no effort made to hide secio node keys because \"IPFS has no interest in [metadata privacy]\" according to Brendan McMillion, so nodes leak their identities onto the raw internet. We think identifying nodes sounds easy anyways, but at minimum this invites attacks. There is an asymmetry to key exchanges that leaks less by first establishing a secure channel and only then authenticating. We might reasonably break this asymmetry by deciding that specific roles require more privacy. We might for example help protect validator operators or improve censorship resistance in some cases, like fishermen. Second, there is cipher suit agility in secio, at minimum in their use of multihash, but maybe even in the underlying key exchange itself. We've seen numerous attacks on TLS <= 1.2 due to cipher suit agility, especially the downgrade attacks. I therefore strongly recommend using TLS 1.3 if cipher suit agility is required. We could likely version the entire protocol though, thus avoiding any cipher suit agility. In any case, constructs like multihash should be considered hazardous in even basic key exchanges, but certainly in more advanced protocols involving signatures or associated data. Third, there are no ACKs in secio which might yield attacks when a higher level protocol actually depends upon the underlying insecure transport's own ACKs. I suppose UDP transport support already requires higher level protocol handle any required ACKs themselves anyways. ( related ). As QUIC uses UDP only, we could add TCP based transport that uses TLS 1.3, perhaps by extending libp2p's existing transport with support for TLS 1.3, or perhaps adding a more flexible TLS 1.3 layer. We might prefer a flexible TLS 1.3 layer over conventional TLS integration into libp2p extending transports because our authentication privacy demands might work differently from TLS's server oriented model. We could identify some reasonable Noise variant , if avoiding the complexity of TLS sounds like a priority and ACKs are always handled by higher layers. I believe Noise XX fits the blockchain context well, due to Alice and Bob roles being easily reversible, improved modularity, and more asynchronous key certification from on-chain data. At the extreme, we could imagine identifying particular handshakes for particular interactions though, like GRANDPA using KK and fishermen using NK. In short, we should make a new years resolution to replace secio, with our two simplest routes being either TLS 1.3 or Noise XX. Aside from these authentication repairs, there are two additional directions for possible future work: Post-quantum key exchange. We'd likely employ LWE scheme here. Right now, CSIDH remains young and slow, but the small key size and long-term keys claims indicate that CSIDH might integrate better with Noise and blockchains. I'd skip the existing specification for integrating Noise with New Hope Simple. Adam Langely has good arguments for selecting the NTRU variant NRSS+SXY for Google's CECPQ2 experiment . I the module-LWE Kyber Forward-security. There is some multi-hop message forwarding in libp2p, but it provides only another addressing technique, not a true connection abstraction layer like say GNUNet's CADET layer. CADET actually employs the Axolotl forward secure ratchet. I'm always a fan of both post-quantum and forward security for encryption, but the benefits might prove minimal in our context.","title":"Transport layer authentication - libp2p's SECIO"},{"location":"keys/creation/","text":"Account key creation ideas for Polkadot We found a trick for using Ed25519 \"mini\" private keys in schnorr-dalek , meaning users' \"mini\" private key consists of 32 bytes of unstructured entropy. There are no serious problems with BIP39 so we suggest a similar strategy for deriving secret keys in Polkadot. We could however modernize BIP39 in a couple small but straightforward ways: Argon2id should replace PBKDF2. Adam Langely sugests using time=2 and 64mb of memopry for interactive scenarios like this. In principle, one might question if this scenario should truly be considered interactive, but conversely one could imagine running this on relatively constrained devices. We might also improve the argone2rs crate too, especially to ensure we use at least v1.3 since v1.2.1 got weaker . Rejection sampling to support larger wordlists. We could employ rejection sampling from the initial entropy stream to avoid tying ourselves to the list size being a power of two, as BIP39 seemingly requires. We can provide roughly the existing error correction from BIP32, even working in a ring of this order. Actually provide a larger wordlist. We're discussing enough entropy that users might benefit form using diceware-like word lists with 12.9 bits of entropy per word, as opposed to BIP32's 11 bits of entropy per word. It's possible some diceware word lists contained confusable words, but reviews exists at least for English. We might worry that larger wordlists might simply not exist for some languges. It's also easier to quickly curate shorter lists. There are also more speculative directions for possible improvements: Improve error correction. Right now BIP39 has only a basic checksum for error correction. We could design schemes that corrected errors by choosing the words using Reed-Solomon, meaning non-systematic word list creation with code words, except naively this limits our word list sizes to finite field sizes, meaning prime powers. We would instead likely run Reed-Solomon separately on each prime power divisor of the word list's order. We should however evaluate alternatives like other generalisations of Reed-Solomon codes to rings , or even working in a field of slightly larger order and reject choices that fall outside the wordlist. Support multiple Argon2id configurations. We might conceivably support multiple argon2id configurations, if small device constraints become a serious concern. We could select among a few argon2id configuration options using yet another output from the Reed-Solomon code. We'd simply use rejection sampling to choose the user's desired configuration.","title":"Account key creation ideas for Polkadot"},{"location":"keys/creation/#account-key-creation-ideas-for-polkadot","text":"We found a trick for using Ed25519 \"mini\" private keys in schnorr-dalek , meaning users' \"mini\" private key consists of 32 bytes of unstructured entropy. There are no serious problems with BIP39 so we suggest a similar strategy for deriving secret keys in Polkadot. We could however modernize BIP39 in a couple small but straightforward ways: Argon2id should replace PBKDF2. Adam Langely sugests using time=2 and 64mb of memopry for interactive scenarios like this. In principle, one might question if this scenario should truly be considered interactive, but conversely one could imagine running this on relatively constrained devices. We might also improve the argone2rs crate too, especially to ensure we use at least v1.3 since v1.2.1 got weaker . Rejection sampling to support larger wordlists. We could employ rejection sampling from the initial entropy stream to avoid tying ourselves to the list size being a power of two, as BIP39 seemingly requires. We can provide roughly the existing error correction from BIP32, even working in a ring of this order. Actually provide a larger wordlist. We're discussing enough entropy that users might benefit form using diceware-like word lists with 12.9 bits of entropy per word, as opposed to BIP32's 11 bits of entropy per word. It's possible some diceware word lists contained confusable words, but reviews exists at least for English. We might worry that larger wordlists might simply not exist for some languges. It's also easier to quickly curate shorter lists. There are also more speculative directions for possible improvements: Improve error correction. Right now BIP39 has only a basic checksum for error correction. We could design schemes that corrected errors by choosing the words using Reed-Solomon, meaning non-systematic word list creation with code words, except naively this limits our word list sizes to finite field sizes, meaning prime powers. We would instead likely run Reed-Solomon separately on each prime power divisor of the word list's order. We should however evaluate alternatives like other generalisations of Reed-Solomon codes to rings , or even working in a field of slightly larger order and reject choices that fall outside the wordlist. Support multiple Argon2id configurations. We might conceivably support multiple argon2id configurations, if small device constraints become a serious concern. We could select among a few argon2id configuration options using yet another output from the Reed-Solomon code. We'd simply use rejection sampling to choose the user's desired configuration.","title":"Account key creation ideas for Polkadot"}]}